{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-up and code for Assignment 3 - Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equation for MRP\n",
    "The value f being in the current state $s$ in an MRP can be decomposed into two parts: the immediate reward $R_{t+1}$, and the discounted value of being in the successor state $\\gamma v(S_{t+1})$. Hence, we can define the value function for state $s$ as:\n",
    "<br>\n",
    "<br>\n",
    "$$v(s) = E[R_{t+1} + \\gamma v(S_{t+1}) ~|~ S_t = s]$$\n",
    "<br>\n",
    "<br>\n",
    "Which then for a discrete state space translates to:\n",
    "<br>\n",
    "<br>\n",
    "$$v(s) = \\mathcal R_s + \\gamma \\sum_{s'\\in\\mathcal S} \\mathcal P_{ss'} v(s')$$\n",
    "<br>\n",
    "<br>\n",
    "Using matrix notation this can be written as:\n",
    "<br>\n",
    "<br>\n",
    "$$v = \\mathcal R + \\gamma \\mathcal P v$$\n",
    "<br>\n",
    "<br>\n",
    "We can then find $v$ by using matrix operations as follows:\n",
    "<br>\n",
    "<br>\n",
    "$$v = (I - \\gamma \\mathcal P)^{-1} \\mathcal R$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_function(mrp: MRP) -> Dict[S, float]:\n",
    "    R, P, index_to_state = get_matrices(mrp)\n",
    "    I = np.identity(R.shape[0])\n",
    "    \n",
    "    v = np.matmul(np.linalg.inv(np.subtract(I, mrp.gamma*P)), R)\n",
    "    vf = {}\n",
    "    for i in range(v.shape[0]):\n",
    "        s = index_to_state[i]\n",
    "        vf[s] = float(v[i])\n",
    "    return vf\n",
    "\n",
    "\n",
    "def get_matrices(mrp: MRP) -> Tuple[np.ndarray, np.ndarray, Dict[int, S]]:\n",
    "    # function to convert reward and transition dicts to matrices\n",
    "    # this is necessary to solve the value funciton using matrix operations\n",
    "    # assumes rewards are defined as a funciton of the current state\n",
    "    sz = len(mrp.mp.States)\n",
    "    R = np.zeros((sz,1))\n",
    "    P = np.zeros((sz,sz))\n",
    "    # keep an index where we map each index in the matrix to the state\n",
    "    index_to_state = {}\n",
    "    \n",
    "    for i, s in enumerate(mrp.mp.States):\n",
    "        index_to_state[i] = s\n",
    "        R[i] = mrp.R[s]\n",
    "        for j, sp in enumerate(mrp.mp.States):\n",
    "            if sp in mrp.mp.P[s].keys():\n",
    "                P[i, j] = mrp.mp.P[s][sp]\n",
    "                \n",
    "    return R, P, index_to_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Decision Process\n",
    "A Markov Decision Process (MDP) consists of a finite set of states $\\mathcal S$, a transition probability matrix $\\mathcal P$ (that now also depends on the action taken), a reward function $\\mathcal R$, a discount factor $\\gamma$, and a finite set of actions $\\mathcal A$. An MDP is then defined by the parameters $\\mathcal S$, $\\mathcal A$, $\\mathcal P$, $\\mathcal R$ and $\\gamma$. An MDP is thus an extension of an MRP.\n",
    "\n",
    "\n",
    "#### Policy\n",
    "A policy $\\pi$ is a probabilistic distribution over actions $a \\in \\mathcal A$ given a state $s$.\n",
    "<br>\n",
    "<br>\n",
    "$$\\pi(a~|~s) = \\mathbb P[A_t = a ~|~ S_t = s]$$\n",
    "<br>\n",
    "- A policy fully defines the behaviour of an agent.\n",
    "- An MDP combined with a policy results in an MRP.\n",
    "\n",
    "\n",
    "#### MDP Value Function\n",
    "We can define the value function for an MDP in two ways, firstly as solely a function of the current state:\n",
    "<br>\n",
    "- The state-value function $v_\\pi(s)$ is the expected aggregated and discounted reward starting in state $s$ and then following policy $\\pi$.\n",
    "<br>\n",
    "<br>\n",
    "$$v_\\pi(s) = \\mathbb E_\\pi[ G_t ~|~ S_t = s]$$\n",
    "<br>\n",
    "and secondly as a function of both the current state and the action (action-value function):\n",
    "<br>\n",
    "- The action-value function $q_\\pi(s,a)$ is the expected aggregated and discounted reward starting in state $s$, taking action $a$ and then following policy $\\pi$.\n",
    "<br>\n",
    "<br>\n",
    "$$q_\\pi(s,a) = \\mathbb E_\\pi[ G_t ~|~ S_t = s, A_t = a]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP(NamedTuple):\n",
    "    States: Set[S]\n",
    "    # the transitions depend on s, a, and s'\n",
    "    # mapping from a state to a mapping of an action to a mapping of a state to a float (probability)\n",
    "    P: Dict[S, Dict[A, Dict[S, float]]]\n",
    "    Actions: A\n",
    "    # reward is a function of the current state,  and the action\n",
    "    R: Dict[S, Dict[A, float]]\n",
    "    gamma: float\n",
    "        \n",
    "        \n",
    "class MDP(NamedTuple):\n",
    "    States: Set[S]\n",
    "    # the transitions depend on s, a, and s'\n",
    "    # mapping from a state to a mapping of an action to a mapping of a state to a float (probability)\n",
    "    P: Dict[S, Dict[A, Dict[S, float]]]\n",
    "    Actions: A\n",
    "    # reward is a function of the current state s, the action a, and the next state sp\n",
    "    R: Dict[S, Dict[A, Dict[S, float]]]\n",
    "    gamma: float\n",
    "        \n",
    "        \n",
    "class Policy(NamedTuple):\n",
    "    # state to action to a probability\n",
    "    pi: Dict[S, Dict[A, float]]\n",
    "        \n",
    "        \n",
    "class state_value_function(NamedTuple):\n",
    "    vf: Dict[S,  float]\n",
    "        \n",
    "\n",
    "class action_value_function(NamedTuple):\n",
    "    vf: Dict[S, Dict[A, float]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the code above we can define the reward in multiple ways. The reward can be a function of the current state and the action taken: $R(s,a)$.\n",
    "The reward can also be a function of the current state, the action taken, and the next state: $r(s,s',a)$. The code below shows how we can convert the latter definition to the former using the laws of expectation:\n",
    "$$R(s,a) = \\sum_{s'\\in \\mathcal S}\\mathcal P_{s,s',a}r(s,s',a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_reward_mdp(mdp: MDP) -> Dict[S, Dict[A, float]]:\n",
    "    # function to convert an mdp using r(s, s', a) to R(s,a)\n",
    "    new_r = {}\n",
    "    for s in mdp.States:\n",
    "        new_r[s] = {}\n",
    "        for a in mdp.R[s]:\n",
    "            new_r[s][a] = 0\n",
    "            for sp in mdp.R[s][a]:\n",
    "                new_r[s][a] += mdp.R[s][a][sp]*mdp.P[s][a][sp]\n",
    "    \n",
    "    return new_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a policy and an MDP we can create an MRP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdp_to_mrp(mdp: MDP, policy: Policy) -> MRP:\n",
    "    # function to convert an MDP and a policy into an MRP\n",
    "    States = mdp.States\n",
    "    gamma = mdp.gamma\n",
    "    reward = {}\n",
    "    probs = {}\n",
    "    for s in States:\n",
    "        # assume the reward is just a function of the current state R(s)\n",
    "        reward[s] = 0\n",
    "        probs[s] = {}\n",
    "        for a in policy[s]:\n",
    "            reward[s] += policy[s][a]*mdp.R[s][a]\n",
    "            for sp in mdp.P[s][a]:\n",
    "                if sp not in probs[s]:\n",
    "                    probs[s][sp] = 0\n",
    "                probs[s][sp] += policy[s][a]*mdp.P[s][a][sp]\n",
    "    \n",
    "    mp = MP(States, probs)\n",
    "    mrp = MRP(mp, reward, gamma)\n",
    "    \n",
    "    return mrp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equations\n",
    "As we saw earlier the state-value function $v_\\pi(s)$ can be expressed as the immediate and future discounted rewards starting in state $s$. We can therefore decompose the state-value function as follows:\n",
    "$$v_\\pi(s) = \\mathbb E[R_{t+1} + \\gamma v_\\pi(S_{t+1})~|~S_t = s]$$\n",
    "<br>\n",
    "Similarly, we can decompose the action-value function into the same two parts:\n",
    "<br> <br>\n",
    "$$q_\\pi(s,a) = \\mathbb E[R_{t+1} + \\gamma q_\\pi(S_{t+1},A_{t+1})~|~S_t = s, A_{t} = a]$$\n",
    "<br> <br>\n",
    "We can see how the state-value function is connected to the action-value function, and how we can transform the latter to the former using the policy $\\pi$:\n",
    "<br> <br>\n",
    "$$v_\\pi(s) = \\sum_{a \\in A}\\pi(a~|~s) q_\\pi(s,a)$$\n",
    "<br> <br>\n",
    "The action-value function also depends on the state-value function. If we take action $a$ in state $s$ the value is the immediate reward, plus the discounted value of the state we end up in when taking action $a$. This can be illustrated by the following equation:\n",
    "<br> <br>\n",
    "$$q_\\pi(s,a) = \\mathcal R_s^a + \\gamma\\sum_{s' \\in S}\\mathcal P_{s,s'}^a v_\\pi(s')$$\n",
    "<br> <br>\n",
    "Combining the last two equations gives us:\n",
    "<br> <br>\n",
    "$$v_\\pi(s) = \\sum_{a \\in A}\\pi(a~|~s)\\Bigg( \\mathcal R_s^a + \\gamma\\sum_{s' \\in S}\\mathcal P_{s,s'}^a v_\\pi(s')\\Bigg)$$\n",
    "<br> <br>\n",
    "We can also combine them into:\n",
    "<br> <br>\n",
    "$$q_\\pi(s,a) = \\mathcal R_s^a + \\gamma\\sum_{s' \\in S}\\mathcal P_{s,s'}^a \\sum_{a' \\in A}\\pi(a'~|~s') q_\\pi(s',a')$$\n",
    "<br> <br>\n",
    "Finally, using matrix notation we can write the state-value function as:\n",
    "<br> <br>\n",
    "$$v_\\pi(s) = \\mathcal R_\\pi + \\gamma\\mathcal P_\\pi v_\\pi$$\n",
    "<br> <br>\n",
    "which can then be transformed into:\n",
    "<br> <br>\n",
    "$$v_\\pi(s) = (I - \\gamma\\mathcal P_\\pi)^{-1}\\mathcal R_\\pi$$\n",
    "<br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Value and Optimal Policy\n",
    "The optimal action-value function is defined as:\n",
    "<br><br>\n",
    "$$q_*(s,a) = max_\\pi q_\\pi(s,a)$$\n",
    "<br><br>\n",
    "We say that $\\pi \\geq \\pi'$ if $v_\\pi(s) \\geq v_{\\pi'}(s)$ for all states $s$. Then,\n",
    "- For any MDP there exist an optimal policy $\\pi_*$ that is better than or equal to all other policies, $\\pi_* \\geq \\pi, \\forall \\pi$.\n",
    "- All optimal policies achieve the optimal state-value function, $v_{\\pi_*}(s) = v_*(s), \\forall s$.\n",
    "- All optimal policies achieve the optimal action-value function, $q_{\\pi_*}(s,a) = q_*(s,a), \\forall s, a$.\n",
    "\n",
    "\n",
    "An optimal policy can be found by maximizing over $q_*(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "We will continue on the example used in the second assignment which was a $4\\times4$ gridworld problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_grid(state: Tuple[int, int], size: int) -> bool:\n",
    "    # helper function to check whether a state is in the grid\n",
    "    return  state[0] >= 0 and state[0] < size and state[1] >= 0 and state[1] < size\n",
    "\n",
    "\n",
    "def get_neighbor_states(state: Tuple[int, int], size: int) -> Set[Tuple[int, int]]:\n",
    "    # function to return a set of neighboring states in the grid\n",
    "    nbr_states = set()\n",
    "    \n",
    "    up_state = s[0]-1, s[1]\n",
    "    if is_in_grid(up_state, size):\n",
    "        nbr_states.add(up_state)\n",
    "        \n",
    "    down_state = s[0]+1, s[1]\n",
    "    if is_in_grid(down_state, size):\n",
    "        nbr_states.add(down_state)\n",
    "        \n",
    "    left_state = s[0], s[1]-1\n",
    "    if is_in_grid(left_state, size):\n",
    "        nbr_states.add(left_state)\n",
    "        \n",
    "    right_state = s[0], s[1]+1\n",
    "    if is_in_grid(right_state, size):\n",
    "        nbr_states.add(right_state)\n",
    "    \n",
    "    return nbr_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the gridworld parameters\n",
    "States = set()\n",
    "P = {}\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        state = (i, j)\n",
    "        States.add(state)\n",
    "\n",
    "for s in States:\n",
    "    P[s] = {}\n",
    "    nbrs = get_neighbor_states(s, 4)\n",
    "    if s == (0,0) or s == (3,3):\n",
    "        P[s][s] = 1.0\n",
    "    else:\n",
    "        for sp in nbrs:\n",
    "            P[s][sp] = 0.25\n",
    "        if len(nbrs) < 4:\n",
    "            P[s][s] = 0.25*(4 - len(nbrs))\n",
    "    \n",
    "        \n",
    "mp = MP(States, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the reward is just a funciton of the current state\n",
    "R = {}\n",
    "for s in mp.States:\n",
    "    if s == (0,0) or s == (3,3):\n",
    "        R[s] = 3.\n",
    "    elif s == (1,2):\n",
    "        R[s] = -2.\n",
    "    else:\n",
    "        R[s] = 0.\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp = MRP(mp, R, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will pull out the value function from the MRP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) 30.000000000000007\n",
      "(0, 1) 13.386087847135503\n",
      "(0, 2) 7.09545337311291\n",
      "(0, 3) 5.805370941637835\n",
      "(1, 0) 13.693968875824414\n",
      "(1, 1) 9.012182544798252\n",
      "(1, 2) 5.248436163060024\n",
      "(1, 3) 7.095453373112911\n",
      "(2, 0) 8.15593247193028\n",
      "(2, 1) 7.725651757527839\n",
      "(2, 2) 9.01218254479825\n",
      "(2, 3) 13.3860878471355\n",
      "(3, 0) 6.673035658852047\n",
      "(3, 1) 8.15593247193028\n",
      "(3, 2) 13.693968875824414\n",
      "(3, 3) 30.000000000000004\n"
     ]
    }
   ],
   "source": [
    "v = get_value_function(mrp)\n",
    "for s in sorted(v):\n",
    "    print(s, v[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top left and bottom right corner states are absorbing but not terminating. These states are also the only states with positive rewards, hence it is reasonable that these states have the highest value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "We will modify the previous example and turn it into an MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_direction(s: S, sp: S) -> int:\n",
    "    # function to figure out in which direction the state sp is from state s\n",
    "    # assume that both states are adjacent\n",
    "    if s[1] > sp[1]:\n",
    "        # sp is to the left of s\n",
    "        return 1\n",
    "    elif s[1] < sp[1]:\n",
    "        # sp is to the right of s\n",
    "        return 2\n",
    "    elif s[0] > sp[0]:\n",
    "        # sp is above s\n",
    "        return 3\n",
    "    elif s[0] < sp[0]:\n",
    "        # sp is below s\n",
    "        return 4\n",
    "    else:\n",
    "        # sp is equal to s\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the gridworld parameters\n",
    "States = set()\n",
    "P_2 = {}\n",
    "A = set()\n",
    "for i in range(4):\n",
    "    # 1 is move left, 2 is move right, 3 is up, 4 is down\n",
    "    A.add(i+1)\n",
    "    for j in range(4):\n",
    "        state = (i, j)\n",
    "        States.add(state)\n",
    "\n",
    "for s in States:\n",
    "    P_2[s] = {}\n",
    "    nbrs = get_neighbor_states(s, 4)\n",
    "    for a in A:\n",
    "        P_2[s][a] = {}\n",
    "        if s == (0,0) or s == (3,3):\n",
    "            P_2[s][a][s] = 1.0\n",
    "        else:\n",
    "            agg_p = 0\n",
    "            for sp in nbrs:\n",
    "                if get_neighbor_direction(s, sp) == a:\n",
    "                    P_2[s][a][sp] = 0.7\n",
    "                    agg_p += 0.7\n",
    "                else:\n",
    "                    P_2[s][a][sp] = 0.1\n",
    "                    agg_p += 0.1\n",
    "            if len(nbrs) < 4:\n",
    "                P_2[s][a][s] = 1. - agg_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the reward is just a function of the current state\n",
    "R_2 = {}\n",
    "for s in States:\n",
    "    R_2[s] = {}\n",
    "    for a in A:\n",
    "        if s == (0,0) or s == (3,3):\n",
    "            R_2[s][a] = 3.\n",
    "        elif s == (1,2):\n",
    "            R_2[s][a] = -2.\n",
    "        else:\n",
    "            R_2[s][a] = 0.\n",
    "gamma_2 = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(States, P_2, A, R_2, gamma_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a policy that always aims to move right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {}\n",
    "for s in mdp.States:\n",
    "    policy[s] = {}\n",
    "    for a in A:\n",
    "        if a == 2:\n",
    "            policy[s][a] = 1.0\n",
    "        else:\n",
    "            policy[s][a] = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the above MDP and policy to create an MRP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp2 = mdp_to_mrp(mdp, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) 30.000000000000007\n",
      "(0, 1) 4.3392109618152865\n",
      "(0, 2) 1.6319135097620148\n",
      "(0, 3) 1.5581165139564894\n",
      "(1, 0) 5.458593761423049\n",
      "(1, 1) 2.4508496011315613\n",
      "(1, 2) 1.2544322614163288\n",
      "(1, 3) 3.215560089213728\n",
      "(2, 0) 8.036500824245438\n",
      "(2, 1) 8.652831681642478\n",
      "(2, 2) 9.568674724791766\n",
      "(2, 3) 10.406976035839175\n",
      "(3, 0) 15.229537245561264\n",
      "(3, 1) 18.674500741552613\n",
      "(3, 2) 23.56251185930879\n",
      "(3, 3) 30.000000000000007\n"
     ]
    }
   ],
   "source": [
    "v2 = get_value_function(mrp2)\n",
    "for s in sorted(v2):\n",
    "    print(s, v2[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "Here is code from previous assignments that are necessary to run the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Any, Dict, Tuple, Set\n",
    "import numpy as np\n",
    "\n",
    "class MP(NamedTuple):\n",
    "    States: Set[S]\n",
    "    P: Dict[S, Dict[S, float]]\n",
    "        \n",
    "        \n",
    "class MRP(NamedTuple):\n",
    "    # assumes that the reward is just a function of the current state\n",
    "    mp: MP\n",
    "    R: Dict[S, float]\n",
    "    gamma: float\n",
    "        \n",
    "        \n",
    "class MRP(NamedTuple):\n",
    "    # assumes that the reward is a function of the transition\n",
    "    mp: MP\n",
    "    R: Dict[S, Dict[S, float]]\n",
    "    gamma: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
