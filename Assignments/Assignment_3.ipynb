{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-up and code for Assignment 3 - Week 2\n",
    "\n",
    "### To do:\n",
    "- ~~Write the Bellman equation for MRP Value Function and code to calculate MRP Value Function (based on Matrix inversion method you learnt in this lecture)~~\n",
    "- ~~Write out the MDP definition, Policy definition and MDP Value Function definition (in LaTeX) in your own style/notation (so you really internalize these concepts)~~\n",
    "- Think about the data structure/class design (in Python 3) to represent MDP, Policy, Value Function, and implement them with clear type definitions\n",
    "- The data struucture/code design of MDP should be incremental (and not independent) to that of MRP\n",
    "- Separately implement the r(s,s',a) and R(s,a) = \\sum_{s'} p(s,s',a) * r(s,s',a) definitions of MDP\n",
    "- Write code to convert/cast the r(s,s',a) definition of MDP to the R(s,a) definition of MDP (put some thought into code design here)\n",
    "- Write code to create a MRP given a MDP and a Policy\n",
    "- Write out all 8 MDP Bellman Equations and also the transformation from Optimal Action-Value function to Optimal Policy (in LaTeX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equation for MRP\n",
    "The value f being in the current state $s$ in an MRP can be decomposed into two parts: the immediate reward $R_{t+1}$, and the discounted value of being in the successor state $\\gamma v(S_{t+1})$. Hence, we can define the value function for state $s$ as:\n",
    "<br>\n",
    "<br>\n",
    "$$v(s) = E[R_{t+1} + \\gamma v(S_{t+1}) ~|~ S_t = s]$$\n",
    "<br>\n",
    "<br>\n",
    "Which then for a discrete state space translates to:\n",
    "<br>\n",
    "<br>\n",
    "$$v(s) = \\mathcal R_s + \\gamma \\sum_{s'\\in\\mathcal S} \\mathcal P_{ss'} v(s')$$\n",
    "<br>\n",
    "<br>\n",
    "Using matrix notation this can be written as:\n",
    "<br>\n",
    "<br>\n",
    "$$v = \\mathcal R + \\gamma \\mathcal P v$$\n",
    "<br>\n",
    "<br>\n",
    "We can then find $v$ by using matrix operations as follows:\n",
    "<br>\n",
    "<br>\n",
    "$$v = (I - \\gamma \\mathcal P)^{-1} \\mathcal R$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_function(mrp: MRP) -> Dict[any, float]:\n",
    "    R, P = get_matrices(mrp)\n",
    "    I = np.identity(r.shape[0])\n",
    "    \n",
    "    v = np.matmul(np.inv(np.subtract(I, mrp.gamma*P)), R)\n",
    "    return v\n",
    "\n",
    "\n",
    "def get_matrices(mrp: MRP) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # function to convert reward and transition dicts to matrices\n",
    "    # this is necessary to solve the value funciton using matrix operations\n",
    "    # assumes rewards are defined as a funciton of the current state\n",
    "    sz = len(mrp.S)\n",
    "    R = np.zeros((sz,1))\n",
    "    P = np.zeros((sz,sz))\n",
    "    \n",
    "    for i, s in enumerate(mrp.S):\n",
    "        R[i] = mrp.R[s]\n",
    "        for j, sp in enumerate(mrp.S):\n",
    "            if sp in mrp.P[s].keys():\n",
    "                P[i, j] = mrp.P[s][sp]\n",
    "                \n",
    "    return R, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Decision Process\n",
    "A Markov Decision Process (MDP) consists of a finite set of states $\\mathcal S$, a transition probability matrix $\\mathcal P$ (that now also depends on the action taken), a reward function $\\mathcal R$, a discount factor $\\gamma$, and a finite set of actions $\\mathcal A$. An MDP is then defined by the parameters $\\mathcal S$, $\\mathcal A$, $\\mathcal P$, $\\mathcal R$ and $\\gamma$. An MDP is thus an extension of an MRP.\n",
    "\n",
    "\n",
    "#### Policy\n",
    "A policy $\\pi$ is a probabilistic distribution over actions $a \\in \\mathcal A$ given a state $s$.\n",
    "<br>\n",
    "<br>\n",
    "$$\\pi(a~|~s) = \\mathbb P[A_t = a ~|~ S_t = s]$$\n",
    "<br>\n",
    "- A policy fully defines the behaviour of an agent.\n",
    "- An MDP combined with a policy results in an MRP.\n",
    "\n",
    "\n",
    "#### MDP Value Function\n",
    "We can define the value function for an MDP in two ways, firstly as solely a function of the current state:\n",
    "<br>\n",
    "- The state-value function $v_\\pi(s)$ is the expected aggregated and discounted reward starting in state $s$ and then following policy $\\pi$.\n",
    "<br>\n",
    "<br>\n",
    "$$v_\\pi(s) = \\mathbb E_\\pi[ G_t ~|~ S_t = s]$$\n",
    "<br>\n",
    "and secondly as a function of both the current state and the action (action-value function):\n",
    "<br>\n",
    "- The action-value function $q_\\pi(s,a)$ is the expected aggregated and discounted reward starting in state $s$, taking action $a$ and then following policy $\\pi$.\n",
    "<br>\n",
    "<br>\n",
    "$$q_\\pi(s,a) = \\mathbb E_\\pi[ G_t ~|~ S_t = s, A_t = a]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Any, Dict, Tuple, Set\n",
    "import numpy as np\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S: Set[any]\n",
    "    # the transitions depend on s, a, and s'\n",
    "    # mapping from a state to a mapping of an action to a mapping of a state to a float (probability)\n",
    "    P: Dict[any, Dict[any, Dict[any, float]]]\n",
    "    A: any\n",
    "    R: Dict[any, float]\n",
    "    gamma: float\n",
    "        \n",
    "        \n",
    "class Policy(NamedTuple):\n",
    "    pi: Dict[any, any]\n",
    "        \n",
    "\n",
    "class value_function(NamedTuple):\n",
    "    pi: Dict[any, any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "Here is code from previous assignments that are necessary to run the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP(NamedTuple):\n",
    "    S: Set[any]\n",
    "    P: Dict[any, Dict[any, float]]\n",
    "        \n",
    "        \n",
    "class MRP(NamedTuple):\n",
    "    # assumes that the reward is just a function of the current state\n",
    "    mp: MP\n",
    "    R: Dict[any, float]\n",
    "    gamma: float\n",
    "        \n",
    "        \n",
    "class MRP(NamedTuple):\n",
    "    # assumes that the reward is a function of the transition\n",
    "    mp: MP\n",
    "    R: Dict[any, Dict[any, float]]\n",
    "    gamma: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
