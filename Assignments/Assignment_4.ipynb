{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-up and code for Jan 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MDP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5e91b6c3ce17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mpolicy_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMDP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstate_value_function\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# implementation of policy evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStates\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mnew_vf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MDP' is not defined"
     ]
    }
   ],
   "source": [
    "def policy_eval(mdp: MDP, policy: Policy, n_iter: int) -> state_value_function:\n",
    "    # implementation of policy evaluation\n",
    "    vf = {s: 0. for s in mdp.States}\n",
    "    for i in range(n_iter):\n",
    "        new_vf = {}\n",
    "        for s in mdp.States:\n",
    "            new_vf[s] = 0\n",
    "            for a in policy[s]:\n",
    "                new_vf[s] += policy[s][a]*mdp.R[s][a]\n",
    "                for sp in mdp.P[s][a]:\n",
    "                    new_vf[s] += policy[s][a]*mdp.gamma*mdp.P[s][a][sp]*vf[sp]\n",
    "        vf = new_vf\n",
    "    return vf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Policy Evaluation\n",
    "We continue in the example from the last assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_grid(state: Tuple[int, int], size: int) -> bool:\n",
    "    # helper function to check whether a state is in the grid\n",
    "    return  state[0] >= 0 and state[0] < size and state[1] >= 0 and state[1] < size\n",
    "\n",
    "\n",
    "def get_neighbor_states(state: Tuple[int, int], size: int) -> Set[Tuple[int, int]]:\n",
    "    # function to return a set of neighboring states in the grid\n",
    "    nbr_states = set()\n",
    "    \n",
    "    up_state = s[0]-1, s[1]\n",
    "    if is_in_grid(up_state, size):\n",
    "        nbr_states.add(up_state)\n",
    "        \n",
    "    down_state = s[0]+1, s[1]\n",
    "    if is_in_grid(down_state, size):\n",
    "        nbr_states.add(down_state)\n",
    "        \n",
    "    left_state = s[0], s[1]-1\n",
    "    if is_in_grid(left_state, size):\n",
    "        nbr_states.add(left_state)\n",
    "        \n",
    "    right_state = s[0], s[1]+1\n",
    "    if is_in_grid(right_state, size):\n",
    "        nbr_states.add(right_state)\n",
    "    \n",
    "    return nbr_states\n",
    "\n",
    "\n",
    "def get_neighbor_direction(s: S, sp: S) -> int:\n",
    "    # function to figure out in which direction the state sp is from state s\n",
    "    # assume that both states are adjacent\n",
    "    if s[1] > sp[1]:\n",
    "        # sp is to the left of s\n",
    "        return 1\n",
    "    elif s[1] < sp[1]:\n",
    "        # sp is to the right of s\n",
    "        return 2\n",
    "    elif s[0] > sp[0]:\n",
    "        # sp is above s\n",
    "        return 3\n",
    "    elif s[0] < sp[0]:\n",
    "        # sp is below s\n",
    "        return 4\n",
    "    else:\n",
    "        # sp is equal to s\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the gridworld parameters\n",
    "States = set()\n",
    "P = {}\n",
    "A = set()\n",
    "for i in range(4):\n",
    "    # 1 is move left, 2 is move right, 3 is up, 4 is down\n",
    "    A.add(i+1)\n",
    "    for j in range(4):\n",
    "        state = (i, j)\n",
    "        States.add(state)\n",
    "\n",
    "for s in States:\n",
    "    P[s] = {}\n",
    "    nbrs = get_neighbor_states(s, 4)\n",
    "    for a in A:\n",
    "        P[s][a] = {}\n",
    "        if s == (0,0) or s == (3,3):\n",
    "            P[s][a][s] = 1.0\n",
    "        else:\n",
    "            agg_p = 0\n",
    "            for sp in nbrs:\n",
    "                if get_neighbor_direction(s, sp) == a:\n",
    "                    P[s][a][sp] = 0.7\n",
    "                    agg_p += 0.7\n",
    "                else:\n",
    "                    P[s][a][sp] = 0.1\n",
    "                    agg_p += 0.1\n",
    "            if len(nbrs) < 4:\n",
    "                P[s][a][s] = 1. - agg_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the reward is just a function of the current state\n",
    "R = {}\n",
    "for s in States:\n",
    "    R[s] = {}\n",
    "    for a in A:\n",
    "        if s == (0,0) or s == (3,3):\n",
    "            R[s][a] = 3.\n",
    "        elif s == (1,2):\n",
    "            R[s][a] = -2.\n",
    "        else:\n",
    "            R[s][a] = 0.\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(States, P, A, R, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {}\n",
    "for s in mdp.States:\n",
    "    policy[s] = {}\n",
    "    for a in A:\n",
    "        if a == 2:\n",
    "            policy[s][a] = 1.0\n",
    "        else:\n",
    "            policy[s][a] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) 19.539646797000003\n",
      "(0, 1) 1.5913445137792583\n",
      "(0, 2) -0.13172112051440504\n",
      "(0, 3) -0.09178616625516067\n",
      "(1, 0) 1.3081295711322876\n",
      "(1, 1) -0.7832546586645136\n",
      "(1, 2) -1.782277895789884\n",
      "(1, 3) 0.20173080195345502\n",
      "(2, 0) 2.229352425175423\n",
      "(2, 1) 2.8046556591403595\n",
      "(2, 2) 3.612483600758209\n",
      "(2, 3) 4.372277018613442\n",
      "(3, 0) 6.560786640943631\n",
      "(3, 1) 9.496772213748145\n",
      "(3, 2) 13.770260155040685\n",
      "(3, 3) 19.539646797000003\n"
     ]
    }
   ],
   "source": [
    "vf = policy_eval(mdp, policy, 10)\n",
    "for s in sorted(vf):\n",
    "    print(s, vf[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) 29.99920315803337\n",
      "(0, 1) 4.338507931135843\n",
      "(0, 2) 1.6312225534949574\n",
      "(0, 3) 1.5574269668684466\n",
      "(1, 0) 5.457869447378502\n",
      "(1, 1) 2.45013666158467\n",
      "(1, 2) 1.2537218449109215\n",
      "(1, 3) 3.214849992506807\n",
      "(2, 0) 8.03575457318074\n",
      "(2, 1) 8.652085375090703\n",
      "(2, 2) 9.567927444793996\n",
      "(2, 3) 10.406227956156409\n",
      "(3, 0) 15.228759092018938\n",
      "(3, 1) 18.673717484120644\n",
      "(3, 2) 23.56172219688203\n",
      "(3, 3) 29.99920315803337\n"
     ]
    }
   ],
   "source": [
    "vf = policy_eval(mdp, policy, 100)\n",
    "for s in sorted(vf):\n",
    "    print(s, vf[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the value function converges. After 10 iterations it differs significantly from the exact value which we saw in the last assignment. After 100 iterations it is very close to the true value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iter(mdp: MDP, policy: Policy, n_iter: int) -> Policy:\n",
    "    for i in range(n_iter):\n",
    "        new_policy = policy\n",
    "        v = policy_eval(mdp, policy, n_iter)\n",
    "        for s in mdp.States:\n",
    "            best_value = -1000000\n",
    "            best_a: A\n",
    "            for a in mdp.P[s]:\n",
    "                # reinitialize the new policy\n",
    "                new_policy[s][a] = 0.\n",
    "                value = mdp.R[s][a]\n",
    "                for sp in mdp.P[s][a]:\n",
    "                    value += mdp.gamma*mdp.P[s][a][sp]*v[sp]\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_a = a\n",
    "            # make the policy deterministic\n",
    "            new_policy[s][best_a] = 1.0\n",
    "                    \n",
    "        policy = new_policy\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Policy Iteration\n",
    "Implement Policy Iteration on the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_policy = policy_iter(mdp, policy, 100)\n",
    "value_policy = policy_eval(mdp, new_policy, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0} 29.99920315803337\n",
      "(0, 1) {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0} 24.915096445321854\n",
      "(0, 2) {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0} 20.729548395641018\n",
      "(0, 3) {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0} 18.20145751300594\n",
      "(1, 0) {1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0} 24.93875485970657\n",
      "(1, 1) {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0} 21.196223380787025\n",
      "(1, 2) {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0} 16.99251986118538\n",
      "(1, 3) {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0} 20.729548395641018\n",
      "(2, 0) {1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0} 20.968761252197577\n",
      "(2, 1) {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0} 19.03557817536556\n",
      "(2, 2) {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0} 21.19622338078703\n",
      "(2, 3) {1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0} 24.915096445321854\n",
      "(3, 0) {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0} 18.411498069982436\n",
      "(3, 1) {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0} 20.968761252197577\n",
      "(3, 2) {1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0} 24.93875485970657\n",
      "(3, 3) {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0} 29.99920315803337\n"
     ]
    }
   ],
   "source": [
    "for s in sorted(new_policy):\n",
    "    print(s, new_policy[s], value_policy[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see how the value of being in each state increases significantly compared to the original policy of just moving right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iter(mdp: MDP, n_iter: int) -> state_value_function:\n",
    "    # implementation of value iteration, the code is very similar to that of policy iteration\n",
    "    # the difference is what kind of information we store\n",
    "    v = {}\n",
    "    for s in mdp.States:\n",
    "        # initialize the value function\n",
    "        v[s] = 0\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # initialize new dictionary to store the values for this iteration\n",
    "        new_v = {}\n",
    "        for s in mdp.States:\n",
    "            # variable to store the best value when looping over the actions\n",
    "            best_value = -1000000\n",
    "            for a in mdp.P[s]:\n",
    "                # variable for storing the value for action a\n",
    "                value = mdp.R[s][a]\n",
    "                for sp in mdp.P[s][a]:\n",
    "                    value += mdp.gamma*mdp.P[s][a][sp]*v[sp]\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "            # store the best value\n",
    "            new_v[s] = best_value\n",
    "        # copy the value function    \n",
    "        v = new_v\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Value Iteration\n",
    "We will once again reuse the previous gridworld example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the value funcitons we get by using value iteration and policy iteration and evaluation\n",
    "value_vi = value_iter(mdp, 100)\n",
    "value_policy = policy_eval(mdp, new_policy, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: (0, 0),  value iteration: 29.9992, policy iter/eval: 29.9992\n",
      "State: (0, 1),  value iteration: 24.9151, policy iter/eval: 24.9151\n",
      "State: (0, 2),  value iteration: 20.7295, policy iter/eval: 20.7295\n",
      "State: (0, 3),  value iteration: 18.2015, policy iter/eval: 18.2015\n",
      "State: (1, 0),  value iteration: 24.9388, policy iter/eval: 24.9388\n",
      "State: (1, 1),  value iteration: 21.1962, policy iter/eval: 21.1962\n",
      "State: (1, 2),  value iteration: 16.9925, policy iter/eval: 16.9925\n",
      "State: (1, 3),  value iteration: 20.7295, policy iter/eval: 20.7295\n",
      "State: (2, 0),  value iteration: 20.9688, policy iter/eval: 20.9688\n",
      "State: (2, 1),  value iteration: 19.0356, policy iter/eval: 19.0356\n",
      "State: (2, 2),  value iteration: 21.1962, policy iter/eval: 21.1962\n",
      "State: (2, 3),  value iteration: 24.9151, policy iter/eval: 24.9151\n",
      "State: (3, 0),  value iteration: 18.4115, policy iter/eval: 18.4115\n",
      "State: (3, 1),  value iteration: 20.9688, policy iter/eval: 20.9688\n",
      "State: (3, 2),  value iteration: 24.9388, policy iter/eval: 24.9388\n",
      "State: (3, 3),  value iteration: 29.9992, policy iter/eval: 29.9992\n"
     ]
    }
   ],
   "source": [
    "for s in sorted(value_vi):\n",
    "    print(\"State: {}, \".format(s), \"value iteration: {0:.4f}, policy iter/eval: {0:.4f}\".format(value_vi[s], value_policy[s]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that both methods produce the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix code\n",
    "Code needed from previous assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Any, Dict, Tuple, Set, Union\n",
    "import numpy as np\n",
    "\n",
    "class MP(NamedTuple):\n",
    "    States: Set[S]\n",
    "    P: Dict[S, Dict[S, float]]\n",
    "        \n",
    "        \n",
    "class MRP(NamedTuple):\n",
    "    mp: MP\n",
    "    R: Union[Dict[S, float], Dict[S, Dict[S, float]]]\n",
    "    gamma: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP(NamedTuple):\n",
    "    States: Set[S]\n",
    "    # the transitions depend on s, a, and s'\n",
    "    # mapping from a state to a mapping of an action to a mapping of a state to a float (probability)\n",
    "    P: Dict[S, Dict[A, Dict[S, float]]]\n",
    "    Actions: A\n",
    "    # reward is a function of the current state,  and the action\n",
    "    R: Union[Dict[S, Dict[A, float]], Dict[S, Dict[A, Dict[S, float]]]]\n",
    "    gamma: float\n",
    "\n",
    "        \n",
    "class Policy(NamedTuple):\n",
    "    # state to action to a probability\n",
    "    pi: Dict[S, Dict[A, float]]\n",
    "        \n",
    "\n",
    "class state_value_function(NamedTuple):\n",
    "    vf: Dict[S,  float]\n",
    "        \n",
    "        \n",
    "class action_value_function(NamedTuple):\n",
    "    vf: Dict[S, Dict[A, float]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
