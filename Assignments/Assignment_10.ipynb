{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-up and code for Feb 13\n",
    "\n",
    "## To Do\n",
    "- ~~Implement Forward-View TD(Lambda) algorithm for Value Function Prediction~~\n",
    "- ~~Implement Backward View TD(Lambda), i.e., Eligibility Traces algorithm for Value Function Prediction~~\n",
    "- Implement these algorithms as offline or online algorithms (offline means updates happen only after a full simulation trace, online means updates happen at every time step)\n",
    "- Test these algorithms on some example MDPs, compare them versus DP Policy Evaluation, and plot their accuracy as a function of Lambda\n",
    "- Prove that Offline Forward-View TD(Lambda) and Offline Backward View TD(Lambda) are equivalent. We covered the proof of Lambda = 1 in class. Do the proof for arbitrary Lambda (similar telescoping argument as done in class) for the case where a state appears only once in an episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward View TD($\\lambda$) for Value Function Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from modules.MDP import MDP, Policy, V\n",
    "from modules.state_action_vars import S, A\n",
    "from modules.RL_prediction import generate_path, action_sampler\n",
    "import numpy as np\n",
    "\n",
    "def n_step_return(return_series: List[float], state_series: List[S], gamma: float, vf: V) -> np.ndarray:\n",
    "    # returns all the n-step returns from 1 up to n\n",
    "    n = len(return_series)\n",
    "    g_n = np.zeros((n,1))\n",
    "    \n",
    "    for i in range(n-1):\n",
    "        g_n[i:] += np.power(gamma, i) * return_series[i]\n",
    "        g_n[i] += np.power(gamma, i+1) * vf[state_series[i]]\n",
    "    g_n[-1] += np.power(gamma, n-1) * return_series[-1]\n",
    "    return g_n\n",
    "\n",
    "\n",
    "def sum_n_step_return(g_n: np.ndarray, lambd: float) -> float:\n",
    "    # take in all the n-step returns and return the lambda summation form the slides\n",
    "    g_lambda = 0\n",
    "    for j, g in enumerate(g_n[:-1]):\n",
    "        g_lambda += np.power(lambd, j) * g\n",
    "        \n",
    "    return (1.0-lambd)*g_lambda + np.power(lambd, g_n.shape[0]-1)*g_n[-1]\n",
    "\n",
    "\n",
    "def lambda_returns(r_series: List[float], state_series: List[S], gamma: float, lambd: float, vf: V) -> float:\n",
    "    # for computational efficiency, combine the calculation of n-step return and the lambda summation\n",
    "    n = len(r_series)\n",
    "    g_lambda = 0\n",
    "    #g_n = np.zeros((n,1))\n",
    "    #g_n = np.multiply(np.power(gamma, np.arange(n)), np.asarray(return_series))\n",
    "    g_n = 0\n",
    "    \n",
    "    for j in range(n-1):\n",
    "        g_n += np.power(gamma, j) * r_series[j]\n",
    "        g_lambda += np.power(lambd, j) * (g_n + np.power(gamma, j+1) * vf[state_series[j]])\n",
    "        \n",
    "    g_n += np.power(gamma, n-1) * r_series[-1]\n",
    "\n",
    "    return (1.0-lambd)*g_lambda + np.power(lambd, n-1)*g_n\n",
    "\n",
    "\n",
    "def forward_view_TD_lambda(policy: Policy, mdp: MDP, lambd: float, num_epi: int, num_steps: int, alpha: float) -> V:\n",
    "    # implementation of forward view TD(lambda) as it is outlined in the lecture slides\n",
    "    v = {}\n",
    "    gamma = mdp.gamma\n",
    "    for s in mdp.States:\n",
    "        v[s] = 0\n",
    "\n",
    "    for i in range(num_epi):\n",
    "        # generate an episode\n",
    "        s_list, _, r_list = generate_path(policy, mdp, num_steps)\n",
    "\n",
    "        for j in range(num_steps):\n",
    "#             g_n = n_step_return(r_list[j:], s_list[j:], gamma, v)\n",
    "#             g_lambd = sum_n_step_return(g_n, lambd)\n",
    "            g_lambd = lambda_returns(r_list[j:], s_list[j:], gamma, lambd, v)\n",
    "            v[s_list[j]] += float(alpha * (g_lambd - v[s_list[j]]))\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward View TD($\\lambda$) for Value Function Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.RL_interface import RL_interface\n",
    "import random\n",
    "\n",
    "def backward_view_TD_lambda(policy: Policy, mdp: MDP, lambd: float, num_epi: int, num_steps: int, alpha: float) -> V:\n",
    "    # implementation of backward view TD(lambda) as it is outlined in the lecture slides\n",
    "\n",
    "    v = {}\n",
    "    # keep a dictionary that maps states to an index, simplifies the book keeping of eligibility traces\n",
    "    state2idx = {}\n",
    "    gamma = mdp.gamma\n",
    "    for i, s in enumerate(mdp.States):\n",
    "        v[s] = 0\n",
    "        state2idx[s] = i\n",
    "\n",
    "    for i in range(num_epi):\n",
    "        # reset the eligibility trace for every episode\n",
    "        E = np.zeros((len(mdp.States),1))\n",
    "        \n",
    "        s = random.sample(mdp.States,1).pop()\n",
    "\n",
    "        for j in range(num_steps):\n",
    "            # update the eligibility traces\n",
    "            E *= gamma*lambd\n",
    "            E[state2idx[s]] += 1.0\n",
    "            \n",
    "            a = action_sampler(policy, s)\n",
    "            sp, r = RL_interface(mdp, s, a)\n",
    "            \n",
    "            delta_t = r + gamma*v[sp] - v[s]\n",
    "            \n",
    "            v = {key: v[key] + float(alpha * delta_t * E[state2idx[key]]) for key in v.keys()}\n",
    "            s = sp\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.gridworld import gridworld\n",
    "from modules.DP import policy_eval, policy_iter, convert_reward, value_iter\n",
    "gw = gridworld(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a policy that always moves to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {}\n",
    "random_policy = {}\n",
    "for s in gw.States:\n",
    "    policy[s] = {}\n",
    "    random_policy[s] = {}\n",
    "    for a in gw.Actions:\n",
    "        random_policy[s][a] = 0.25\n",
    "        if a == 2:\n",
    "            policy[s][a] = 1.0\n",
    "        else:\n",
    "            policy[s][a] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy_gridworld(policy: Policy):\n",
    "    # function that prints out the grid\n",
    "    last_s = (0,0)\n",
    "    for s in sorted(policy.keys()):\n",
    "        if s[0] != last_s[0]:\n",
    "            print()\n",
    "        for a in policy[s]:\n",
    "            \n",
    "            if np.abs(policy[s][a] - 1.0) < 1e-6:\n",
    "                if a == 1:\n",
    "                    string = '<-'\n",
    "                elif a == 2:\n",
    "                    string = '->'\n",
    "                elif a == 3:\n",
    "                    string = '/\\\\'\n",
    "                else:\n",
    "                    string = '\\\\/'\n",
    "                print(s, \": {} \\t\".format(string), end='')\n",
    "        last_s = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = policy_iter(gw, policy, 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) : <- \t(0, 1) : <- \t(0, 2) : <- \t(0, 3) : <- \t\n",
      "(1, 0) : /\\ \t(1, 1) : <- \t(1, 2) : \\/ \t(1, 3) : \\/ \t\n",
      "(2, 0) : /\\ \t(2, 1) : <- \t(2, 2) : \\/ \t(2, 3) : \\/ \t\n",
      "(3, 0) : -> \t(3, 1) : -> \t(3, 2) : -> \t(3, 3) : <- \t"
     ]
    }
   ],
   "source": [
    "print_policy_gridworld(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the policy using Policy Evaluation, and confirm it is the optimal by comparing it to Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_pe = policy_eval(gw, policy, 1e-5)\n",
    "vf_vi = value_iter(gw, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the policy using Forward View TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_fv = forward_view_TD_lambda(policy, gw, 0.85, 5000, 30, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the policy using Backward View TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_bv = backward_view_TD_lambda(policy, gw, 0.9, 10000, 30, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the three methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) Value Iteration: 0.000 \tPolicy Evaluation: 0.000 \tForward View: 0.000 \tBackward View: 0.000\n",
      "(0, 1) Value Iteration: 2.564 \tPolicy Evaluation: 2.564 \tForward View: 2.867 \tBackward View: 2.561\n",
      "(0, 2) Value Iteration: 1.562 \tPolicy Evaluation: 1.562 \tForward View: 1.625 \tBackward View: 1.605\n",
      "(0, 3) Value Iteration: 1.190 \tPolicy Evaluation: 1.190 \tForward View: 1.237 \tBackward View: 1.211\n",
      "(1, 0) Value Iteration: 2.586 \tPolicy Evaluation: 2.586 \tForward View: 2.915 \tBackward View: 2.562\n",
      "(1, 1) Value Iteration: 1.674 \tPolicy Evaluation: 1.674 \tForward View: 1.831 \tBackward View: 1.679\n",
      "(1, 2) Value Iteration: 1.321 \tPolicy Evaluation: 1.321 \tForward View: 1.376 \tBackward View: 1.327\n",
      "(1, 3) Value Iteration: 1.562 \tPolicy Evaluation: 1.562 \tForward View: 1.728 \tBackward View: 1.525\n",
      "(2, 0) Value Iteration: 1.819 \tPolicy Evaluation: 1.819 \tForward View: 1.923 \tBackward View: 1.817\n",
      "(2, 1) Value Iteration: 1.432 \tPolicy Evaluation: 1.432 \tForward View: 1.456 \tBackward View: 1.432\n",
      "(2, 2) Value Iteration: 1.674 \tPolicy Evaluation: 1.674 \tForward View: 1.820 \tBackward View: 1.666\n",
      "(2, 3) Value Iteration: 2.564 \tPolicy Evaluation: 2.564 \tForward View: 2.941 \tBackward View: 2.485\n",
      "(3, 0) Value Iteration: 1.386 \tPolicy Evaluation: 1.386 \tForward View: 1.419 \tBackward View: 1.313\n",
      "(3, 1) Value Iteration: 1.819 \tPolicy Evaluation: 1.819 \tForward View: 1.828 \tBackward View: 1.791\n",
      "(3, 2) Value Iteration: 2.586 \tPolicy Evaluation: 2.586 \tForward View: 2.780 \tBackward View: 2.504\n",
      "(3, 3) Value Iteration: 0.000 \tPolicy Evaluation: 0.000 \tForward View: 0.000 \tBackward View: 0.000\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(vf_pe.keys()):\n",
    "    print(key, 'Value Iteration: {:0.3f} \\tPolicy Evaluation: {:0.3f} \\tForward View: {:0.3f} \\tBackward View: {:0.3f}'.format(vf_vi[key], vf_pe[key], vf_fv[key], vf_bv[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let their accuracy be the average squared distance over all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_squared_dist(vf_true: V, vf_pred: V) -> float:\n",
    "    # return the squared differences between all states\n",
    "    dist = 0\n",
    "    for s in vf_true:\n",
    "        dist += np.square(vf_true[s] - vf_pred[s])\n",
    "    \n",
    "    return dist/len(vf_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02967827523690294 0.0014788881265040102\n"
     ]
    }
   ],
   "source": [
    "fv_dist = average_squared_dist(vf_pe, vf_fv)\n",
    "bv_dist = average_squared_dist(vf_pe, vf_bv)\n",
    "print(fv_dist, bv_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2302ac31543f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfv_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbv_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlambd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_view_TD_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_view_TD_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "fv_acc = []\n",
    "bv_acc = []\n",
    "for lambd in range(0,1,0.1):\n",
    "    fv = forward_view_TD_lambda(policy, gw, lambd, 5000, 30, 0.01)\n",
    "    bv = backward_view_TD_lambda(policy, gw, lambd, 5000, 30, 0.01)\n",
    "    fv_acc.append(average_squared_dist(vf_pe, fv))\n",
    "    bv_acc.append(average_squared_dist(vf_pe, bv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_pe_random = policy_eval(gw, random_policy, 1e-5)\n",
    "vf_fv_random = forward_view_TD_lambda(random_policy, gw, 0.85, 5000, 30, 0.01)\n",
    "vf_bv_random = backward_view_TD_lambda(random_policy, gw, 0.85, 5000, 30, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) Policy Evaluation: 0.000 \tForward View: 0.000 \tBackward View: 0.000\n",
      "(0, 1) Policy Evaluation: 0.721 \tForward View: 0.773 \tBackward View: 0.731\n",
      "(0, 2) Policy Evaluation: -0.638 \tForward View: -0.824 \tBackward View: -0.726\n",
      "(0, 3) Policy Evaluation: -0.425 \tForward View: -0.464 \tBackward View: -0.401\n",
      "(1, 0) Policy Evaluation: 0.954 \tForward View: 0.923 \tBackward View: 0.973\n",
      "(1, 1) Policy Evaluation: -0.229 \tForward View: -0.420 \tBackward View: -0.273\n",
      "(1, 2) Policy Evaluation: -0.347 \tForward View: -0.430 \tBackward View: -0.279\n",
      "(1, 3) Policy Evaluation: -0.638 \tForward View: -0.598 \tBackward View: -0.609\n",
      "(2, 0) Policy Evaluation: 0.294 \tForward View: 0.257 \tBackward View: 0.310\n",
      "(2, 1) Policy Evaluation: 0.026 \tForward View: -0.041 \tBackward View: -0.040\n",
      "(2, 2) Policy Evaluation: -0.229 \tForward View: -0.305 \tBackward View: -0.196\n",
      "(2, 3) Policy Evaluation: 0.721 \tForward View: 0.798 \tBackward View: 0.619\n",
      "(3, 0) Policy Evaluation: 0.196 \tForward View: 0.134 \tBackward View: 0.278\n",
      "(3, 1) Policy Evaluation: 0.294 \tForward View: 0.294 \tBackward View: 0.374\n",
      "(3, 2) Policy Evaluation: 0.954 \tForward View: 1.060 \tBackward View: 1.060\n",
      "(3, 3) Policy Evaluation: 0.000 \tForward View: 0.000 \tBackward View: 0.000\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(vf_pe_random.keys()):\n",
    "    print(key, 'Policy Evaluation: {:0.3f} \\tForward View: {:0.3f} \\tBackward View: {:0.3f}'.format(vf_pe_random[key], vf_fv_random[key], vf_bv_random[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00733754865054568 0.003520974310142502\n"
     ]
    }
   ],
   "source": [
    "fv_dist_random = average_squared_dist(vf_pe_random, vf_fv_random)\n",
    "bv_dist_random = average_squared_dist(vf_pe_random, vf_bv_random)\n",
    "print(fv_dist_random, bv_dist_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
