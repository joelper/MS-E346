{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write up and code for Assignment 2 - Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "#### Markov Process\n",
    "A Markov Process (MP) is a stochastic process where each state follow the Markov property, i.e. $\\mathbb P[S_{t+1}=s'~|~S_1, S_2, ..., S_t] = \\mathbb P[S_{t+1} = s' ~|~ S_t]$. This means that history beyond the current state offers no more information about the transition to the next state. The transition probabilities are governed by a transition probability matrix $\\mathcal P$. The states in an MP consists of a finite set $\\mathcal S$. An MP is then defined by its parameters $\\mathcal S$ and $\\mathcal P$.\n",
    "\n",
    "#### Markov Reward Process\n",
    "In addition to a finite set of states $\\mathcal S$, and a transition probability matrix $\\mathcal P$, a Markov Reward Process (MRP) also has a reward function $\\mathcal R$, ($\\mathcal R_s = \\mathbb E[R_{t+1}~|~S_t = s]$), and a discount factor $\\gamma \\in [0,1]$. An MRP is then defined by the parameters $\\mathcal S$, $\\mathcal P$, $\\mathcal R$ and $\\gamma$. An MRP is thus an extension of an MP.\n",
    "\n",
    "#### Value function\n",
    "First we define the return $G_t$ as the total discounted rewards recieved starting at time $t$. $$G_t = R_{t} + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + ...$$\n",
    "\n",
    "The state value function $v(s)$ is the value of an MDP when starting in state $s$. It is given by:\n",
    "$$v(s) = \\mathbb E[G_t~|~S_t = s]$$\n",
    "Using the definition of the return we can write the value function as $$v(s) = \\mathbb E[R_t + \\gamma R_{t+1} + ... ~|~S_t = s] = \\mathbb E[R_t ~|~ S_t = s] + \\gamma \\mathbb E[G_{t+1}~|~S_t = s]$$\n",
    "\n",
    "\n",
    "### Data Structures\n",
    "Below is the code for how to represent the above processes in code as structs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Any, Dict, Tuple, Set\n",
    "import numpy as np\n",
    "\n",
    "class MP(NamedTuple):\n",
    "    States: Set[S]\n",
    "    P: Dict[S, Dict[S, float]]\n",
    "        \n",
    "        \n",
    "class MRP(NamedTuple):\n",
    "    # assumes that the reward is just a function of the current state\n",
    "    mp: MP\n",
    "    R: Dict[S, float]\n",
    "    gamma: float\n",
    "        \n",
    "        \n",
    "class MRP(NamedTuple):\n",
    "    # assumes that the reward is a function of the transition\n",
    "    mp: MP\n",
    "    R: Dict[S, Dict[S, float]]\n",
    "    gamma: float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Functions\n",
    "The reward function for a particular movement can be defined in mulitple ways, it can either be a function of the state the Agent leaves and the new state the Agent enters, i.e. $R(s,s')$. Or the reward function can be a function of simply the current state, i.e. $R(s)$. The latter reward function is then simply an expected value of the reward for the next transition: $$R(s) = \\mathbb E[R(s,s')~|~ S = s] = \\sum_{s'\\in \\mathcal S}P(S_{t+1} = s'~|~S_t=s)\\times R(s, s')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_reward(R: Dict[S, Dict[S, float]], P: Dict[S, Dict[S, float]]) -> Dict[S, float]:\n",
    "    # this function converts reward as a function of the transition to reward as a function of the current state\n",
    "    new_R: {}\n",
    "    for s, d in P.items():\n",
    "        # assume that the first element in the key tuple is the current state\n",
    "        if s not in new_R:\n",
    "            new_R[s] = 0\n",
    "        for sp, p in d.items():\n",
    "            new_R[s] += p * R[s][sp]\n",
    "        \n",
    "    return new_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary Distribution\n",
    "Below is code to find the stationary distribution of a Markov Process. The stationary distirbution can be found analytically using eigenvalues and eigenvectors of the transition matrix. In this problem I've chosen to find the stationary distribution of a Markov Process using simulation. By simulating the process a large number of times, and to a sufficient depth and then count how many times we end up in every state we can estimate the stationary distribution. For a large set of states we need a large number of simulations to get a reasonable estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_stationary_dist(mp: MP, n_iter: int, depth: int) -> Dict[S, float]:\n",
    "    # this function finds a stationary distribution for a Markov Process through simulation\n",
    "    stat_dist = {}\n",
    "    for i in range(n_iter):\n",
    "        state = random.sample(mp.States, 1).pop()\n",
    "        for j in range(depth):\n",
    "            new_state = get_random_state(mp.P[state])\n",
    "            state = new_state\n",
    "        \n",
    "        if state not in stat_dist:\n",
    "            stat_dist[state] = 0\n",
    "        stat_dist[state] += 1/n_iter\n",
    "    return stat_dist\n",
    "\n",
    "\n",
    "def get_random_state(d: Dict[S, float]) -> S:\n",
    "    # helper function to return a random state based on a mapping from a state to a float\n",
    "    p = np.random.rand()\n",
    "    agg_prob = 0\n",
    "    \n",
    "    for sp, prob in d.items():\n",
    "        agg_prob += prob\n",
    "        if p <= agg_prob:\n",
    "            return sp\n",
    "    # degenerate distribution if it does not return a state before reaching this point\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I've used the eigenvalues approach to finding the stationary distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import eig\n",
    "\n",
    "def get_stat_dist_eig(mp: MP) -> Dict[S, float]:\n",
    "    # create a matrix to store the transition probabilities\n",
    "    mat = np.zeros((len(mp.States), len(mp.States)))\n",
    "    for i, s in enumerate(mp.States):\n",
    "        for j, sp in enumerate(mp.States):\n",
    "            if sp in mp.P[s]:\n",
    "                # if we have a mapping from s -> {sp -> a probability} then the transition has non-zero probability\n",
    "                mat[i,j] = mp.P[s][sp]\n",
    "                \n",
    "    v = get_valid_eig(mat)\n",
    "    dist = {}\n",
    "    for i, s in enumerate(mp.States):\n",
    "        dist[s] = v[i]\n",
    "        \n",
    "    return dist\n",
    "\n",
    "\n",
    "def get_valid_eig(mat: np.array) -> np.array:\n",
    "    # helper function to return a normalized valid eigenvector (i.e. no negative numbers)\n",
    "    eig_vals, eig_vecs = eig(mat.T)\n",
    "    v = np.zeros((eig_vals.shape))\n",
    "    # we use the eigenvector associated with an eigenvalue = 1\n",
    "    for i, val in enumerate(eig_vals):\n",
    "        if np.abs(val - 1.) < 10 ** (-5):\n",
    "            v = eig_vecs[:, i]\n",
    "            break\n",
    "            \n",
    "    return v / sum(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "Below is a simple example of a gridworld environment, we have a 4$\\times$4 grid where in each cell you have probability of 0.25 of going in any direction (north, south, east, west), unless if you are at the border. If you are at the rightmost edge you will have 0.25 chance of going north, south, west and a probability of 0.25 of staying in the same cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_grid(state: Tuple[int, int], size: int) -> bool:\n",
    "    # helper function to check whether a state is in the grid\n",
    "    return  state[0] >= 0 and state[0] < size and state[1] >= 0 and state[1] < size\n",
    "\n",
    "\n",
    "def get_neighbor_states(state: Tuple[int, int], size: int) -> Set[Tuple[int, int]]:\n",
    "    # function to return a set of neighboring states in the grid\n",
    "    nbr_states = set()\n",
    "    \n",
    "    up_state = s[0]-1, s[1]\n",
    "    if is_in_grid(up_state, size):\n",
    "        nbr_states.add(up_state)\n",
    "        \n",
    "    down_state = s[0]+1, s[1]\n",
    "    if is_in_grid(down_state, size):\n",
    "        nbr_states.add(down_state)\n",
    "        \n",
    "    left_state = s[0], s[1]-1\n",
    "    if is_in_grid(left_state, size):\n",
    "        nbr_states.add(left_state)\n",
    "        \n",
    "    right_state = s[0], s[1]+1\n",
    "    if is_in_grid(right_state, size):\n",
    "        nbr_states.add(right_state)\n",
    "    \n",
    "    return nbr_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the gridworld parameters\n",
    "States = set()\n",
    "P = {}\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        state = (i, j)\n",
    "        States.add(state)\n",
    "\n",
    "for s in States:\n",
    "    P[s] = {}\n",
    "    nbrs = get_neighbor_states(s, 4)\n",
    "    for sp in nbrs:\n",
    "        P[s][sp] = 0.25\n",
    "    if len(nbrs) < 4:\n",
    "        P[s][s] = 0.25*(4 - len(nbrs))\n",
    "        \n",
    "mp = MP(States, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can evaluate how the simulation approach of finding the stationary distribution compares to the analytical approach of using eigenvalues. We observe that they are fairly similar. We can expect the simulaiton method to approach the analytical solution as the number of simulaitons increases, in this case I chose 10,000 simulations to a depth of 100, i.e. every simulation was done for 100 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MP' object has no attribute 'S'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3ab79b1fc3af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_stationary_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-77336f6d2634>\u001b[0m in \u001b[0;36mget_stationary_dist\u001b[0;34m(mp, n_iter, depth)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstat_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MP' object has no attribute 'S'"
     ]
    }
   ],
   "source": [
    "get_stationary_dist(mp, 10000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.06250000000000018,\n",
       " (0, 1): 0.06250000000000008,\n",
       " (0, 2): 0.06250000000000018,\n",
       " (0, 3): 0.06250000000000015,\n",
       " (1, 0): 0.06250000000000003,\n",
       " (1, 1): 0.06250000000000014,\n",
       " (1, 2): 0.06250000000000014,\n",
       " (1, 3): 0.06250000000000008,\n",
       " (2, 0): 0.06250000000000001,\n",
       " (2, 1): 0.06249999999999996,\n",
       " (2, 2): 0.06249999999999997,\n",
       " (2, 3): 0.06249999999999991,\n",
       " (3, 0): 0.06249999999999997,\n",
       " (3, 1): 0.06249999999999983,\n",
       " (3, 2): 0.062499999999999806,\n",
       " (3, 3): 0.06249999999999974}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stat_dist_eig(mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
