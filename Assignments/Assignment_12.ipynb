{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-up and code for Feb 20 and 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP Interface for RL Algorithms with Value Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, NamedTuple, Callable, Set, Tuple, List\n",
    "from modules.MDP import MDP, Q, Policy, V\n",
    "from modules.RL_interface import RL_interface\n",
    "from modules.state_action_vars import S, A\n",
    "import random\n",
    "import numpy as np\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_interface_FA(NamedTuple):\n",
    "    # interface for reinforcement learning with value function approximation, \n",
    "    # largely inspired by Professor Ashwin Rao's implementation\n",
    "    \n",
    "    # function that takes in a state and return a set of possible actions\n",
    "    state_action_func: Callable[[S], Set[A]]\n",
    "    # function that takes in a state and an action, and returns a new state sp and the reward \n",
    "    state_reward_func: Callable[[S, A], Tuple[S, float]]\n",
    "    # initial state generator\n",
    "    init_state_gen: Callable[[], S]\n",
    "    # inital state generator\n",
    "    init_state_action_gen: Callable[[], Tuple[S, A]]\n",
    "    # discount factor\n",
    "    gamma: float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Prediction Algorithm with Value Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_vi_approx(polf: Callable[[S], A], alpha: float, v_hat: Callable[[S, np.ndarray], float], \n",
    "                 sampler: RL_interface_FA, num_epi: int, num_steps: int, d: int) \\\n",
    "    -> Callable[[S], float]:\n",
    "    # implementation of Monte-Carlo Prediction Algorithm with Value Function Approximation\n",
    "    # assume the approximation function is linear\n",
    "\n",
    "    # initialize weight vector\n",
    "    w = np.zeros((d,1))\n",
    "    \n",
    "    gamma = sampler.gamma\n",
    "    \n",
    "    for i in range(num_epi):\n",
    "        s_list, a_list, r_list = get_mc_path(polf, sampler, num_steps)\n",
    "        G = 0\n",
    "        for j in range(num_steps):\n",
    "            G = np.sum(np.multiply(np.power(gamma, np.arange(num_steps-j)), np.array(r_list[j:])))\n",
    "            w += alpha*(G - v_hat(s_list[j], w))*w\n",
    "   \n",
    "    return w\n",
    "    \n",
    "    \n",
    "def get_mc_path(polf: Callable[[S], A], sampler: RL_interface_FA, num_steps: int) \\\n",
    "    -> Tuple[List[S], List[A], List[float]]:\n",
    "    # simulate a Monte-Carlo path\n",
    "    s_list = []\n",
    "    a_list = []\n",
    "    r_list = []\n",
    "\n",
    "    s, a = sampler.init_state_action_gen()\n",
    "    s_list.append(s)\n",
    "    a_list.append(a)\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        s, r = sampler.state_reward_func(s, a)\n",
    "        a = polf(s)\n",
    "        s_list.append(s)\n",
    "        a_list.append(a)\n",
    "        r_list.append(r)\n",
    "        \n",
    "    # sample the last reward\n",
    "    _, r = sampler.state_reward_func(s, a)\n",
    "    r_list.append(r)\n",
    "    \n",
    "    return s_list, a_list, r_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-step TD Prediction with Value Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_0_approx(polf: Callable[[S], A], alpha: float, v_hat: Callable[[S, np.ndarray], float], \n",
    "                 sampler: RL_interface_FA, num_epi: int, num_steps: int, d: int) \\\n",
    "    -> Callable[[S], float]:\n",
    "    # implementation of 1-step TD prediction with Value Function Approximation\n",
    "    # assume the approximation function is linear\n",
    "\n",
    "    # initialize weight vector\n",
    "    w = np.zeros((d,1))\n",
    "    \n",
    "    gamma = sampler.gamma\n",
    "    \n",
    "    for i in range(num_epi):\n",
    "        s = sampler.init_state_gen()\n",
    "        G = 0\n",
    "        for j in range(num_steps):\n",
    "            # sample a state from the policy\n",
    "            a = polf(s)\n",
    "            # observe next state and the reward\n",
    "            sp, r = sampler.state_reward_func(s, a)\n",
    "            \n",
    "            w += alpha*(r + gamma*v_hat(sp, w) - v_hat(s, w))*w\n",
    "            s = sp\n",
    "   \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eligibility Traces based TD($\\lambda$) with Value Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_td_lambda_approx(polf: Callable[[S], A], alpha: float, v_hat: Callable[[S, np.ndarray], float], \n",
    "                     sampler: RL_interface_FA, num_epi: int, num_steps: int, d: int, \n",
    "                    feature_func: Callable[[S], np.ndarray], lam: float) \\\n",
    "    -> Callable[[S], float]:\n",
    "    # implementation of Backward View TD(lambda) Prediction Algorithm with Value Function Approximation\n",
    "    # assume the approximation function is linear\n",
    "\n",
    "    # initialize weight vector\n",
    "    w = np.zeros((d,1))\n",
    "    \n",
    "    gamma = sampler.gamma\n",
    "    \n",
    "    for i in range(num_epi):\n",
    "        # initialize eligibility traces\n",
    "        E = np.zeros((d,1))\n",
    "        s = sampler.init_state_gen()\n",
    "        \n",
    "        G = 0\n",
    "        for j in range(num_steps):\n",
    "            # sample a state from the policy\n",
    "            a = polf(s)\n",
    "            # observe next state and the reward\n",
    "            sp, r = sampler.state_reward_func(s, a)\n",
    "            \n",
    "            delta = r + gamma*v_hat(sp, w) - v_hat(s, w)\n",
    "            E += gamma*lam*E + feature_func(s)\n",
    "            w += alpha*delta*E\n",
    "            \n",
    "            s = sp\n",
    "   \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA and SARSA($\\lambda$) with Value Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_fa(feature_func: Callable[[S, A], np.ndarray], polf: Callable[[S, np.ndarray, float], A],\n",
    "             alpha: float, sampler: RL_interface_FA, num_epi: int, num_steps: int, d: int, eps: float) \\\n",
    "    -> np.ndarray:\n",
    "    # implementation of Sarsa with Value Function Approximation\n",
    "    # assume the approximation function is linear    \n",
    "    \n",
    "    # initialize weight vector\n",
    "    w = np.zeros((d,1))\n",
    "    \n",
    "    gamma = sampler.gamma\n",
    "    for i in range(num_epi):\n",
    "        s,a = sampler.init_state_action_gen()\n",
    "        for t in range(num_steps):\n",
    "            # observe next state and the reward\n",
    "            sp, r = sampler.state_reward_func(s, a)\n",
    "            # take next action ap using epsilon greedy policy\n",
    "            ap = polf(sp, w, eps)\n",
    "            # find Q(sp, ap; w) and Q(s, a; w)\n",
    "            q = np.dot(feature_func(s,a), w)\n",
    "            q_next = np.dot(feature_func(sp,ap), w)\n",
    "            # calculate delta\n",
    "            delta = r + gamma*q_next - q\n",
    "            # update w\n",
    "            w += alpha*delta*feature_func(s,a)\n",
    "            # update s and a\n",
    "            s, a = sp, ap\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_lambda_fa(feature_func: Callable[[S, A], np.ndarray], polf: Callable[[S, np.ndarray, float], A], \n",
    "            alpha: float, sampler: RL_interface_FA, num_epi: int, num_steps: int, d: int, eps: float) \\\n",
    "    -> np.ndarray:\n",
    "    # implementation of Sarsa(lambda) with Value Function Approximation\n",
    "    # assume the approximation function is linear    \n",
    "    \n",
    "    # initialize weight vector\n",
    "    w = np.zeros((d,1))\n",
    "    \n",
    "    gamma = sampler.gamma\n",
    "    for i in range(num_epi):\n",
    "        E = np.zeros((d,1))\n",
    "        s,a = sampler.init_state_action_gen()\n",
    "    \n",
    "        G = 0\n",
    "        for t in range(num_steps):\n",
    "            # observe next state and the reward\n",
    "            sp, r = sampler.state_reward_func(s, a)\n",
    "            # take next action ap using epsilon greedy\n",
    "            ap = polf(sp, w, eps)\n",
    "            # find Q(sp, ap; w) and Q(s, a; w)\n",
    "            q = np.dot(feature_func(s,a), w)\n",
    "            q_next = np.dot(feature_func(sp,ap), w)\n",
    "            # calculate delta\n",
    "            delta = r + gamma*q_next - q\n",
    "            # update eligibility traces\n",
    "            E += gamma*lam*E + feature_func(s,a)\n",
    "            # update weights\n",
    "            w += alpha*delta*E\n",
    "            \n",
    "            s = sp\n",
    "            a = ap\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning with Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning_fa(feature_func: Callable[[S, A], np.ndarray], polf: Callable[[S, np.ndarray, float], A],\n",
    "             alpha: float, sampler: RL_interface_FA, num_epi: int, num_steps: int, d: int, eps: float,\n",
    "                q_max_finder: Callable[[S, np.ndarray], float]) \\\n",
    "    -> np.ndarray:\n",
    "    # implementation of Q-learning with Value Function Approximation\n",
    "    # assume the approximation function is linear    \n",
    "    \n",
    "    # initialize weight vector\n",
    "    w = np.zeros((d,1))\n",
    "    \n",
    "    gamma = sampler.gamma\n",
    "    for i in range(num_epi):\n",
    "        s = sampler.init_state_gen()\n",
    "        for t in range(num_steps):\n",
    "            # take action a, sampled from a epsilon greedy policy\n",
    "            a = polf(s, w, eps)\n",
    "            # observe next state and the reward\n",
    "            sp, r = sampler.state_reward_func(s, a)\n",
    "            # find Q(s, a; w)\n",
    "            q = np.dot(feature_func(s,a), w)\n",
    "            # find best Q for next state\n",
    "            q_max = q_max_finder(sp, w)\n",
    "            # calculate delta\n",
    "            delta = r + gamma*q_max - q\n",
    "            # update w\n",
    "            w += alpha*delta*feature_func(s,a)\n",
    "            # update s\n",
    "            s = sp\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Square Policy Iteration for American Option Pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.Option import Option, monte_carlo_stock, payoff, longstaff_schwartz, Binary_Tree\n",
    "\n",
    "def lspi(option: Option, m: int, n: int, r: int) -> float:\n",
    "    # simulate stock paths\n",
    "    SP = monte_carlo_stock(option, m, n)\n",
    "    # initialize parameters\n",
    "    A = np.zeros((r,r))\n",
    "    B = np.zeros((r,1))\n",
    "    w = np.zeros((r,1))\n",
    "    delta_t = option.tau / n\n",
    "    for i in range(m):\n",
    "        time = 0\n",
    "        for j in range(n):\n",
    "            Q = payoff(SP[i,j+1], option)\n",
    "            phi = feature_func(SP[i,j], option, time)\n",
    "            phi_next = feature_func(SP[i,j+1], option, time + delta_t)\n",
    "            \n",
    "            P = np.zeros((1,r))\n",
    "            if j < n-1 and Q <= np.matmul(phi_next,w):\n",
    "                P = phi_next\n",
    "            \n",
    "            R = 0\n",
    "            if Q > np.matmul(P,w):\n",
    "                R = Q\n",
    "            \n",
    "            A += np.matmul(phi.T, np.subtract(phi, np.exp(-option.r*delta_t) * P))\n",
    "            B += np.exp(-option.r*delta_t) * R * phi.T\n",
    "            time += delta_t\n",
    "        if (i+1) % 100 == 0 or (i+1) == m:     \n",
    "            w = np.matmul(np.linalg.inv(A), B)\n",
    "            A = np.zeros((r,r))\n",
    "            B = np.zeros((r,1))\n",
    "        if (i) % 10000 == 0 and i > 0:\n",
    "            print(\"Price after {} iterations: \".format(i), np.matmul(feature_func(option.S, option, 0), w)[0][0])\n",
    "    \n",
    "    return np.matmul(feature_func(option.S, option, 0), w)[0][0]\n",
    "\n",
    "\n",
    "def feature_func(s: float, option: Option, time: float) -> np.ndarray:\n",
    "        sp = s / option.K\n",
    "        ttm = option.tau - time\n",
    "        phi0 = 1.0\n",
    "        phi1 = np.exp(-sp / 2.0)\n",
    "        phi2 = phi1 * (1.0 - sp)\n",
    "        phi3 = phi1 * (1.0 - 2.0*sp + np.square(sp)/2.0)\n",
    "        phi_t_0 = np.sin(-time*np.pi/(2.0*option.tau)+np.pi/2.0)\n",
    "        phi_t_1 = np.log(ttm)\n",
    "        phi_t_2 = np.square(time/option.tau)\n",
    "        features = np.array((phi0, phi1, phi2, phi3, phi_t_0, phi_t_1, phi_t_2)).reshape(1, 7)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_put = Option(False, True, 36., 40., 0.2, 1.0, 0.06, 0)\n",
    "am_put_2 = Option(False, True, 100., 110., 0.25, 0.5, 0.05, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price after 10000 iterations:  4.471473257563076\n",
      "Price after 20000 iterations:  4.829348800646807\n",
      "Price after 30000 iterations:  4.608171980580508\n",
      "Price after 40000 iterations:  4.88816264779814\n",
      "Price after 50000 iterations:  4.649290890093857\n",
      "Price after 60000 iterations:  4.559197012090703\n",
      "Price after 70000 iterations:  4.772003005452177\n",
      "Price after 80000 iterations:  4.873095222981682\n",
      "Price after 90000 iterations:  4.320027732771136\n",
      "Price for American put using LSPI:  4.575048600610511\n",
      "Price for American put using Longstaff-Schwartz:  4.642148884469446\n",
      "Price for American put using binary tree:  4.490003213687527\n",
      "Time:  395.3375598449493\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "print(\"Price for American put using LSPI: \", lspi(am_put, 100000, 100, 7))\n",
    "print(\"Price for American put using Longstaff-Schwartz: \", longstaff_schwartz(am_put, 1000, 100))\n",
    "print(\"Price for American put using binary tree: \", Binary_Tree(am_put, 20))\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price after 0 iterations:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price after 1000 iterations:  12.6058723869879\n",
      "Price after 2000 iterations:  11.770321257150504\n",
      "Price after 3000 iterations:  11.965668510449419\n",
      "Price after 4000 iterations:  11.592394066477201\n",
      "Price after 5000 iterations:  13.213612152520172\n",
      "Price after 6000 iterations:  12.940232198231497\n",
      "Price after 7000 iterations:  14.346933230949892\n",
      "Price after 8000 iterations:  12.714963702592684\n",
      "Price after 9000 iterations:  13.082343755210312\n",
      "Price for American put using LSPI:  11.950302912033951\n",
      "Price for American put using Longstaff-Schwartz:  12.403651641369288\n",
      "Price for American put using binary tree:  12.151536447011493\n"
     ]
    }
   ],
   "source": [
    "print(\"Price for American put using LSPI: \", lspi(am_put_2, 10000, 100, 7))\n",
    "print(\"Price for American put using Longstaff-Schwartz: \", longstaff_schwartz(am_put_2, 1000, 100))\n",
    "print(\"Price for American put using binary tree: \", Binary_Tree(am_put_2, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
