{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-up and code for Feb 15\n",
    "\n",
    "## To Do:\n",
    "- ~~Prove the Epsilon-Greedy Policy Improvement Theorem (we sketched the proof in Class)~~\n",
    "- ~~Provide (with clear mathematical notation) the defintion of GLIE (Greedy in the Limit with Infinite Exploration)~~\n",
    "- ~~Implement the tabular SARSA and tabular SARSA(Lambda) algorithms~~\n",
    "- ~~Implement the tabular Q-Learning algorithm~~\n",
    "- ~~Test the above algorithms on some example MDPs by using DP Policy Iteration/Value Iteration solutions as a benchmark~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-Greedy Policy Improvement Theorem\n",
    "__Theorem:__ For any $\\epsilon$-greedy policy $\\pi$, the $\\epsilon$-greedy policy $\\pi'$ with respect to $q_\\pi$ is an improvement, $v_{\\pi'}(s) \\geq v_\\pi(s)$\n",
    "\n",
    "__Proof:__ \n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\pi(s,\\pi'(s)) & = \\sum_{a\\in\\mathcal A}\\pi'(a|s)q_\\pi(s,a)\\\\\n",
    "& = \\epsilon/m\\sum_{a\\in\\mathcal A}q_\\pi(s,a) + (1-\\epsilon) \\max_{a \\in\\mathcal A}q_\\pi(s,a)\\\\\n",
    "& \\geq \\epsilon/m\\sum_{a\\in\\mathcal A}q_\\pi(s,a) + (1-\\epsilon) \\sum_{a \\in\\mathcal A}\\frac{\\pi(a|s) - \\epsilon/m}{1-\\epsilon}q_\\pi(s,a)\\\\\n",
    "& = \\sum_{a\\in\\mathcal A}\\pi(a|s)q_\\pi(s,a)\\\\\n",
    "& = v_\\pi(s)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, from policy improvement theorem $v_{\\pi'}(s) \\geq v_\\pi(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy in the Limit with Infinite Exploration\n",
    "__Definition:__ Greedy in the Limit with Infinite Exploration (GLIE)\n",
    "- All State-Action pairs are explored infinitely many times,\n",
    "\n",
    "$$\n",
    "\\lim_{k\\to\\infty}N_k(s,a) = \\infty\n",
    "$$\n",
    "\n",
    "where $N_k(s,a)$ is the number of times we have taken action $a$ and visited state $s$ after $k$ time-steps\n",
    "\n",
    "- The policy converges on a greedy policy,\n",
    "\n",
    "$$\n",
    "\\lim_{k\\to\\infty}\\pi(a|s) = \\mathbf 1\\big(a=\\arg\\max_{a'\\in\\mathcal A}Q_k(s,a')\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa and Sarsa($\\lambda$) Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from modules.MDP import MDP, Q, Policy\n",
    "from modules.RL_interface import RL_interface\n",
    "from modules.state_action_vars import S, A\n",
    "import random\n",
    "\n",
    "def sarsa(mdp: MDP, num_epi: int, num_steps: int, eps: float, alpha: float) -> Q:\n",
    "    # implementation of Sarsa-learning\n",
    "    \n",
    "    # initialize Q(s,a) to zero for all state-action pairs\n",
    "    q = {s: {a: 0. for a in mdp.Actions} for s in mdp.States}\n",
    "    \n",
    "    for i in range(num_epi):\n",
    "        # sample a random starting state and an action\n",
    "        s = random.sample(mdp.States,1).pop()\n",
    "        if random.random() > eps:\n",
    "            _, a = find_max_q(q, s)\n",
    "        else:\n",
    "            a = random.sample(mdp.Actions,1).pop()\n",
    "        \n",
    "        for j in range(num_steps):\n",
    "            # observe a reward r and a next state sp\n",
    "            sp, r = RL_interface(mdp, s, a)\n",
    "        \n",
    "            # follow an epsilon-greedy policy\n",
    "            if random.random() > eps:\n",
    "                _, ap = find_max_q(q, sp)\n",
    "            else:\n",
    "                ap = random.sample(mdp.Actions,1).pop()\n",
    "            \n",
    "            # update the q-function\n",
    "            q[s][a] += alpha*(r + mdp.gamma*q[sp][ap] - q[s][a])\n",
    "            \n",
    "            s = sp\n",
    "            a = ap\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_lambda(mdp: MDP, num_epi: int, num_steps: int, eps: float, alpha: float, lambd: float) -> Q:\n",
    "    # implementation of the Sarsa-lambda algorithm\n",
    "    \n",
    "    # initialize Q(s,a) and N(s,a) to zero for all state-action pairs\n",
    "    q = {s: {a: 0. for a in mdp.Actions} for s in mdp.States}\n",
    "    n = {s: {a: 0 for a in mdp.Actions} for s in mdp.States}\n",
    "    \n",
    "    for i in range(num_epi):\n",
    "        # sample a random starting state and an action\n",
    "        s = random.sample(mdp.States,1).pop()\n",
    "        if random.random() > eps:\n",
    "            _, a = find_max_q(q, s)\n",
    "        else:\n",
    "            a = random.sample(mdp.Actions,1).pop()\n",
    "        \n",
    "        for j in range(num_steps):\n",
    "            # observe a reward r and a next state sp\n",
    "            sp, r = RL_interface(mdp, s, a)\n",
    "            # follow an epsilon-greedy policy\n",
    "            if random.random() > eps:\n",
    "                _, ap = find_max_q(q, sp)\n",
    "            else:\n",
    "                ap = random.sample(mdp.Actions,1).pop()\n",
    "            # increment the eligibility trace\n",
    "            n[s][a] += 1\n",
    "            # calculate the error\n",
    "            delta = r + mdp.gamma*q[sp][ap] - q[s][a]\n",
    "            \n",
    "            for s_ in mdp.States:\n",
    "                for a_ in mdp.Actions:\n",
    "                    # update the q-function\n",
    "                    q[s_][a_] += alpha*delta*n[s_][a_]\n",
    "                    n[s_][a_] *= mdp.gamma*lambd\n",
    "            \n",
    "            s = sp\n",
    "            a = ap\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_q(q: Q, s: S) -> A:\n",
    "    # returns the best action for a specific state\n",
    "    best_value = -1e7\n",
    "    best_a = None\n",
    "    \n",
    "    # loop over all the actions and store Q(s,a) and a if it is the current best action\n",
    "    for a in q[s]:\n",
    "        if q[s][a] > best_value:\n",
    "            best_value = q[s][a]\n",
    "            best_a = a\n",
    "            \n",
    "    return best_value, best_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(mdp: MDP, num_epi: int, num_steps: int, eps: float, alpha: float) -> Q:\n",
    "    # implementation of Sarsa-learning\n",
    "    \n",
    "    # initialize Q(s,a) to zero for all state-action pairs\n",
    "    q = {s: {a: 0. for a in mdp.Actions} for s in mdp.States}\n",
    "    \n",
    "    for i in range(num_epi):\n",
    "        # sample a random starting state and an action\n",
    "        s = random.sample(mdp.States,1).pop()\n",
    "        \n",
    "        for j in range(num_steps):\n",
    "            if random.random() > eps:\n",
    "                _, a = find_max_q(q, s)\n",
    "            else:\n",
    "                a = random.sample(mdp.Actions,1).pop()\n",
    "            # observe a reward r and a next state sp\n",
    "            sp, r = RL_interface(mdp, s, a)\n",
    "            # find the max for next state\n",
    "            q_max = find_max_q(q, sp)\n",
    "            # update the q-function\n",
    "            q[s][a] += alpha*(r + mdp.gamma*q_max - q[s][a])\n",
    "            \n",
    "            s = sp\n",
    "            a = ap\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_to_policy(q: Q) -> Policy:\n",
    "    # takes in Q(s,a) and returns a greedy policy\n",
    "    policy = {s: {a: 0.0 for a in q[s]} for s in q}\n",
    "    \n",
    "    for s in q:\n",
    "        _, a = find_max_q(q, s)\n",
    "        policy[s][a] = 1.0\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld Example\n",
    "Continue on the previous Gridworld example. Remember, there are positive rewards of entering state (0,0) and (3,3) while there is a negative reward when entering state (1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.gridworld import gridworld\n",
    "import numpy as np\n",
    "gw = gridworld(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy_gridworld(policy: Policy):\n",
    "    # function that prints out the grid\n",
    "    last_s = (0,0)\n",
    "    for s in sorted(policy.keys()):\n",
    "        if s[0] != last_s[0]:\n",
    "            print()\n",
    "        for a in policy[s]:\n",
    "            \n",
    "            if np.abs(policy[s][a] - 1.0) < 1e-6:\n",
    "                if a == 1:\n",
    "                    string = '<-'\n",
    "                elif a == 2:\n",
    "                    string = '->'\n",
    "                elif a == 3:\n",
    "                    string = '/\\\\'\n",
    "                else:\n",
    "                    string = '\\\\/'\n",
    "                print(s, \": {} \\t\".format(string), end='')\n",
    "        last_s = s\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_sarsa = sarsa(gw, 1000, 30, 0.3, 0.01)\n",
    "q_sarsa_lambda = sarsa_lambda(gw, 10000, 30, 0.3, 0.01, 0.9)\n",
    "q_learn = sarsa(gw, 1000, 30, 0.3, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_sarsa = q_to_policy(q_sarsa)\n",
    "policy_sarsa_lambda = q_to_policy(q_sarsa_lambda)\n",
    "policy_qlearn = q_to_policy(q_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) : <- \t(0, 1) : <- \t(0, 2) : <- \t(0, 3) : <- \t\n",
      "(1, 0) : /\\ \t(1, 1) : /\\ \t(1, 2) : <- \t(1, 3) : \\/ \t\n",
      "(2, 0) : /\\ \t(2, 1) : <- \t(2, 2) : \\/ \t(2, 3) : <- \t\n",
      "(3, 0) : /\\ \t(3, 1) : -> \t(3, 2) : -> \t(3, 3) : <- \t\n",
      "\n",
      "(0, 0) : <- \t(0, 1) : <- \t(0, 2) : <- \t(0, 3) : <- \t\n",
      "(1, 0) : /\\ \t(1, 1) : <- \t(1, 2) : <- \t(1, 3) : \\/ \t\n",
      "(2, 0) : /\\ \t(2, 1) : /\\ \t(2, 2) : \\/ \t(2, 3) : \\/ \t\n",
      "(3, 0) : /\\ \t(3, 1) : -> \t(3, 2) : -> \t(3, 3) : -> \t\n",
      "\n",
      "(0, 0) : <- \t(0, 1) : <- \t(0, 2) : <- \t(0, 3) : <- \t\n",
      "(1, 0) : /\\ \t(1, 1) : /\\ \t(1, 2) : /\\ \t(1, 3) : \\/ \t\n",
      "(2, 0) : /\\ \t(2, 1) : <- \t(2, 2) : \\/ \t(2, 3) : \\/ \t\n",
      "(3, 0) : /\\ \t(3, 1) : -> \t(3, 2) : -> \t(3, 3) : <- \t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_policy_gridworld(policy_sarsa)\n",
    "print_policy_gridworld(policy_sarsa_lambda)\n",
    "print_policy_gridworld(policy_qlearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Jack's Car Rental\n",
    "The number of cars available in the afternoon at $t+1$ in location A is given by:\n",
    "\n",
    "$$\n",
    "A_{t+1} = A_t + N^A_{t+1} - M^A_{t+1} + D_t\n",
    "$$\n",
    "\n",
    "where $N^A_t$ is the number of cars returned to A, $M^A_t$ is the number of cars rented at A, and $D_t$ is the number of cars moved between A and B. The equation is similar for location B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
