{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-up and code for Feb 27 and Mar 1\n",
    "\n",
    "## To Do:\n",
    "- ~~Write with proper notation, the derivations to solutions of Linear Systems for Bellman Error-minimization and Projected Bellman Error-minimization (lecture slides have the derivation, but aim to do it yourself)~~\n",
    "- Optional: Write with proper notation, the derivation of Gradient TD (the last two slides of the lecture have the derivation, but aim to do it yourself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Systems for Bellman Error-minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bellman Error (BE)-minimizing function approximation weight vector $w_{BE}$ is defined as\n",
    "\n",
    "$$w_{BE} = \\arg\\min_w d(B_\\pi v_w, v_w)$$\n",
    "- Where $d(v_1, v_2)$ is the 'weighted distance' between vectors $v_1$ and $v_2$, defined as\n",
    "$$d(v_1, v_2) = \\sum_{i=1}^{n}\\mu_\\pi(s_i)\\cdot(v_1(s_i) - v_2(s_i))^2 = (v_1 - v_2)^\\top \\cdot D\\cdot (v_1 - v_2) $$\n",
    "- where $\\mu_\\pi(\\cdot)$ is the stationary distribution and $D$ is a diagonal matrix where $D_{ii} = \\mu_\\pi(s_i)$\n",
    "- We have that $v_w = \\Phi w$, i.e. the value function approximation vector\n",
    "- We also have that $B_\\pi\\cdot v = R_\\pi + \\gamma P_\\pi \\cdot v$\n",
    "Using the above property we can write $w_{BE}$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_{BE} & = \\arg\\min_w d(B_\\pi v_w, v_w)\\\\\n",
    "& = \\arg\\min_w d\\big(R_\\pi + \\gamma P_\\pi \\cdot v_w,~ v_w\\big)\\\\\n",
    "& = \\arg\\min_w d\\big(R_\\pi + \\gamma P_\\pi \\cdot \\Phi \\cdot w,~ \\Phi\\cdot w\\big)\\\\\n",
    "& = \\arg\\min_w d\\big(R_\\pi,~ (\\Phi - \\gamma P_\\pi \\cdot \\Phi) \\cdot w\\big)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This formulation equates to a weighted least squares regression where we are regressing $\\Phi - \\gamma P_\\pi \\cdot \\Phi$ on $R_\\pi$ with weight matrix $D$, hence the solution is given by\n",
    "$$\n",
    "w_{BE} = \\big((\\Phi - \\gamma P_\\pi \\cdot \\Phi)^\\top\\cdot D\\cdot(\\Phi - \\gamma P_\\pi \\cdot \\Phi)\\big)^{-1}\\cdot\\big(\\Phi - \\gamma P_\\pi \\cdot \\Phi\\big)^\\top \\cdot D\\cdot R_\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Systems for Projected Bellman Error (PBE)-minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Projected Bellman Error (PBE)-minimizing function approximation weight vector $w_{PBE}$ is defined as\n",
    "\n",
    "$$w_{PBE} = \\arg\\min_w d\\big((\\Pi_\\Phi \\cdot B_\\pi)\\cdot v_w,~ v_w\\big)$$\n",
    "\n",
    "- The minimum is 0, i.e. $\\Phi\\cdot w_{PBE}$ is the fixed point of operator $\\Pi_\\Phi\\cdot B_\\pi$\n",
    "- If we start with an arbitrary value function vector $v$ and then repeatedly apply the Bellman Operator $B_\\pi$ followed by $\\Pi_\\Phi$, we will eventually reach the fixed point $\\Phi\\cdot w_{PBE}$. \n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "(\\Pi_\\Phi \\cdot B_\\pi)\\cdot \\Phi\\cdot w_{PBE} = \\Phi\\cdot w_{PBE}\n",
    "$$\n",
    "\n",
    "- We know that $\\Pi_\\Phi = \\Phi\\cdot(\\Phi^\\top\\cdot D\\cdot \\Phi)^{-1}\\cdot\\Phi^\\top\\cdot D$\n",
    "- and as before that $B_\\pi v = R_\\pi + \\gamma P_\\pi v$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "(\\Pi_\\Phi \\cdot B_\\pi)\\cdot \\Phi\\cdot w_{PBE} = \\Phi\\cdot(\\Phi^\\top\\cdot D\\cdot \\Phi)^{-1}\\cdot\\Phi^\\top\\cdot D \\cdot (R_\\pi + \\gamma P_\\pi \\cdot \\Phi\\cdot w_{PBE}) = \\Phi\\cdot w_{PBE}\n",
    "$$\n",
    "\n",
    "Assuming $\\Phi$ is a full rank matrix gives us \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "(\\Phi^\\top\\cdot D\\cdot \\Phi)^{-1}\\cdot\\Phi^\\top\\cdot D \\cdot (R_\\pi + \\gamma P_\\pi \\cdot \\Phi\\cdot w_{PBE}) & = w_{PBE}\\\\\n",
    "\\iff\\\\\n",
    "\\Phi^\\top\\cdot D \\cdot (R_\\pi + \\gamma P_\\pi \\cdot \\Phi\\cdot w_{PBE}) & = (\\Phi^\\top\\cdot D\\cdot \\Phi)\\cdot w_{PBE}\\\\\n",
    "\\iff\\\\\n",
    "\\Phi^\\top\\cdot D\\cdot \\Phi\\cdot w_{PBE} - \\Phi^\\top\\cdot D \\cdot\\gamma P_\\pi \\cdot \\Phi\\cdot w_{PBE} & = \\Phi^\\top\\cdot D \\cdot R_\\pi\\\\\n",
    "\\iff\\\\\n",
    "\\Phi^\\top\\cdot D \\cdot (\\Phi - \\gamma P_\\pi \\cdot \\Phi)\\cdot w_{PBE} & = \\Phi^\\top\\cdot D \\cdot R_\\pi\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Which is a square linear system of the form $A\\cdot w_{PBE} = b$, the solution is given by\n",
    "\n",
    "$$\n",
    "w_{PBE} = A^{-1}\\cdot b = \\big(\\Phi^\\top\\cdot D \\cdot (\\Phi - \\gamma P_\\pi \\cdot \\Phi) \\big)^{-1}\\cdot\\Phi^\\top\\cdot D\\cdot R_\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of Gradient TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
