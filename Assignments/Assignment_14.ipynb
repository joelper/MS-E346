{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-up and code for Mar 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of Policy Gradient Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy $\\pi$ is a function of both the state and the parameter $\\theta$ hence we have $\\pi(a~|~s, \\theta)$. Take the gradient of the state-value function with respect to the parameter $\\theta$\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\int_{\\mathcal S}p_0(s_o)\\cdot v_\\pi(s_0)\\cdot ds_0 = \\int_{\\mathcal S}p_0(s_o)\\cdot \\int_{\\mathcal A}\\pi(a_0, s_0; \\theta) \\cdot q_\\pi(s_0,a_0)\\cdot da_0 \\cdot ds_0\n",
    "$$\n",
    "\n",
    "Calculate $\\nabla J(\\theta)$ by parts $\\pi(a_0, s_0;\\theta)$ and $q_\\pi(s_0, a_0)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla J(\\theta) & = \\int_{\\mathcal S}p_0(s_o)\\cdot \\int_{\\mathcal A}\\nabla\\pi(a_0, s_0; \\theta) \\cdot q_\\pi(s_0,a_0)\\cdot da_0 \\cdot ds_0\\\\\n",
    "& + \\int_{\\mathcal S}p_0(s_o)\\cdot \\int_{\\mathcal A}\\pi(a_0, s_0; \\theta) \\cdot \\nabla q_\\pi(s_0,a_0)\\cdot da_0 \\cdot ds_0\\\\\n",
    "\\\\\n",
    "& \\text{Use Bellman to expand } q_\\pi\\\\\n",
    "\\\\\n",
    "& = \\int_{\\mathcal S}p_0(s_o)\\cdot \\int_{\\mathcal A}\\nabla\\pi(a_0, s_0; \\theta) \\cdot q_\\pi(s_0,a_0)\\cdot da_0 \\cdot ds_0 \\\\\n",
    "& + \\int_{\\mathcal S}p_0(s_o)\\cdot \\int_{\\mathcal A}\\pi(a_0, s_0; \\theta) \\cdot \\nabla  \\Big(\\mathcal R_{s_0}^{a_0} + \\int_{\\mathcal S}\\gamma\\mathcal P_{s_0,s_1}^{a_0}\\cdot v_\\pi(s_1)\\cdot ds_1 \\Big) \\cdot da_0 \\cdot ds_0\\\\\n",
    "\\\\\n",
    "& \\text{The gradient w.r.t. the reward is 0}\\\\\n",
    "\\\\\n",
    "& = \\int_{\\mathcal S}p_0(s_o)\\cdot \\int_{\\mathcal A}\\nabla\\pi(a_0, s_0; \\theta) \\cdot q_\\pi(s_0,a_0)\\cdot da_0 \\cdot ds_0 \\\\\n",
    "& + \\int_{\\mathcal S}p_0(s_o)\\cdot \\int_{\\mathcal A}\\pi(a_0, s_0; \\theta) \\cdot \\nabla  \\Big(\\int_{\\mathcal S}\\gamma\\mathcal P_{s_0,s_1}^{a_0}\\cdot v_\\pi(s_1)\\cdot ds_1 \\Big) \\cdot da_0 \\cdot ds_0\\\\\n",
    "\\\\\n",
    "& \\text{Only the value function depends on $\\theta$}\\\\\n",
    "\\\\\n",
    "& = \\int_{\\mathcal S}p_0(s_o)\\cdot \\int_{\\mathcal A}\\nabla\\pi(a_0, s_0; \\theta) \\cdot q_\\pi(s_0,a_0)\\cdot da_0 \\cdot ds_0 \\\\\n",
    "& + \\int_{\\mathcal S}p_0(s_o)\\cdot \\int_{\\mathcal A}\\pi(a_0, s_0; \\theta) \\cdot \\int_{\\mathcal S}\\gamma\\mathcal P_{s_0,s_1}^{a_0}\\cdot \\nabla v_\\pi(s_1)\\cdot ds_1 \\cdot da_0 \\cdot ds_0 \\\\\n",
    "\\\\\n",
    "& \\text{Note that } \\int_{\\mathcal A} \\pi(s_0, a_0;\\theta)\\mathcal P_{s_0,s_1}^{a_0} \\cdot da_0 = Pr(s_0\\rightarrow s_1, 1, \\pi) \\\\\n",
    "& \\text{which is the probability of transitioning from $s_0$ to $s_1$ in one step under policy $\\pi$}\\\\\n",
    "\\\\\n",
    "& = \\int_{\\mathcal S}p_0(s_o)\\cdot \\int_{\\mathcal A}\\nabla\\pi(a_0, s_0; \\theta) \\cdot q_\\pi(s_0,a_0)\\cdot da_0 \\cdot ds_0 \\\\\n",
    "& + \\int_{\\mathcal S}\\big( \\int_{\\mathcal S}\\gamma \\cdot p_0(s_o) \\cdot Pr(s_0\\rightarrow s_1, 1, \\pi)\\cdot ds_0 \\big)\\nabla v_\\pi(s_1)\\cdot ds_1 \\\\\n",
    "\\\\\n",
    "& \\text{Once again, expand $v_\\pi$}\\\\\n",
    "\\\\\n",
    "& = \\int_{\\mathcal S}p_0(s_o)\\cdot \\int_{\\mathcal A}\\nabla\\pi(a_0, s_0; \\theta) \\cdot q_\\pi(s_0,a_0)\\cdot da_0 \\cdot ds_0 \\\\\n",
    "& + \\int_{\\mathcal S}\\big( \\int_{\\mathcal S}\\gamma \\cdot p_0(s_o) \\cdot Pr(s_0\\rightarrow s_1, 1, \\pi)\\cdot ds_0 \\big)\\nabla \\Big(\\int_{\\mathcal A} \\pi(a_1, s_1; \\theta) \\cdot q_\\pi(s_1,a_1)\\cdot da_1 \\Big)\\cdot ds_1 \\\\\n",
    "\\\\\n",
    "& \\text{We are now back to a similar scenario as where we started, where we need to take the gradient by parts on $\\pi$ and $q_\\pi$}\\\\\n",
    "& \\text{telescoping these equations to infinity gives us}\\\\\n",
    "\\\\\n",
    "& = \\sum_{t=0}^\\infty \\int_{\\mathcal S}\\int_{\\mathcal S}\\gamma^t\\cdot p_0(s_0)\\cdot Pr(s_0\\rightarrow s_t, t, \\pi)\\cdot ds_0\\int_{\\mathcal A}\\nabla \\pi(s_t,a_t;\\theta)\\cdot q_\\pi(s_t, a_t) \\cdot da_t\\cdot ds_t\\\\\n",
    "\\\\\n",
    "& = \\int_{\\mathcal S}\\int_{\\mathcal S}\\sum_{t=0}^\\infty \\gamma^t\\cdot p_0(s_0)\\cdot Pr(s_0\\rightarrow s, t, \\pi)\\cdot ds_0\\int_{\\mathcal A}\\nabla \\pi(s,a;\\theta)\\cdot q_\\pi(s, a) \\cdot da\\cdot ds\\\\\n",
    "\\\\\n",
    "& \\text{Note that by definition $\\int_{\\mathcal S}\\sum_{t=0}^\\infty \\gamma^t\\cdot p_0(s_0)\\cdot Pr(s_0\\rightarrow s, t, \\pi)\\cdot ds_0 = \\rho_\\pi(s)$}\\\\\n",
    "\\\\\n",
    "& = \\int_{\\mathcal S}\\rho_\\pi(s)\\int_{\\mathcal A}\\nabla \\pi(s,a;\\theta)\\cdot q_\\pi(s, a) \\cdot da\\cdot ds\\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive the score function for softmax policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score function is defined as $\\nabla_\\theta \\log \\pi(s,a;\\theta)$. A softmax policy is given by\n",
    "\n",
    "$$\n",
    "\\pi(s,a;\\theta) = \\frac{e^{\\theta^\\top \\phi(s,a)}}{\\sum_{a'} e^{\\theta^\\top \\phi(s,a')}}\n",
    "$$\n",
    "\n",
    "Find the score function of a softmax policy\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\log\\pi(s,a;\\theta) & = \\nabla_\\theta \\log \\Big(\\frac{e^{\\theta^\\top \\phi(s,a)}}{\\sum_{a'} e^{\\theta^\\top \\phi(s,a')}}\\Big) \\\\\n",
    "\\\\\n",
    "& = \\nabla_\\theta \\log e^{\\theta^\\top \\phi(s,a)} - \\nabla_\\theta \\log \\sum_{a'} e^{\\theta^\\top \\phi(s,a')}\\\\\n",
    "\\\\\n",
    "& = \\phi(s,a) - \\frac{1}{\\sum_{a'} e^{\\theta^\\top \\phi(s,a')}}\\cdot \\sum_{a''} e^{\\theta^\\top \\phi(s,a'')} \\cdot \\phi(s,a'')\\\\\n",
    "\\\\\n",
    "& = \\phi(s,a) - \\sum_{a''} \\frac{e^{\\theta^\\top \\phi(s,a'')}}{\\sum_{a'} e^{\\theta^\\top \\phi(s,a')}} \\cdot \\phi(s,a'')\\\\\n",
    "\\\\\n",
    "& = \\phi(s,a) - \\sum_{a''} \\pi(s,a'';\\theta)\\cdot\\phi(s,a'')\\\\\n",
    "\\\\\n",
    "& = \\phi(s,a) - \\mathbb E_\\pi[\\phi(s,\\cdot)]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive the score function for Gaussian policy\n",
    "A Gaussian policy is given by\n",
    "\n",
    "$$\n",
    "a \\sim \\mathcal N(\\theta^\\top\\cdot\\phi(s), \\sigma^2),~~ \\forall s \\in \\mathcal S\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\pi(s,a;\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\big(-\\frac{(a - \\theta^\\top\\cdot\\phi(s))^2}{2\\sigma^2}\\big)\n",
    "$$\n",
    "\n",
    "Find the score function of a Gaussian policy\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\log\\pi(s,a;\\theta) & = \\nabla_\\theta \\log \\Big(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\big(-\\frac{(a - \\theta^\\top\\cdot\\phi(s))^2}{2\\sigma^2}\\big)\\Big) \\\\\n",
    "\\\\\n",
    "& = \\nabla_\\theta \\Big(-\\frac{(a - \\theta^\\top\\cdot\\phi(s))^2}{2\\sigma^2} \\Big)\\\\\n",
    "\\\\\n",
    "& = \\frac{a - \\theta^\\top\\cdot\\phi(s)}{\\sigma^2}\\cdot \\phi(s)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE Algoithm (Monte-Carlo Policy Gradient Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from modules.RL_interface_FA import RL_interface_FA\n",
    "from modules.state_action_vars import S, A\n",
    "from typing import Callable, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(sampler: RL_interface_FA, polf: Callable[[S, np.ndarray], A], \n",
    "              polf_grad: Callable[[S, A, np.ndarray], np.ndarray], d: int,\n",
    "              num_epi: int, num_steps: int, alpha: float) -> np.ndarray:\n",
    "    # reinforce algorithm\n",
    "    \n",
    "    # initialize theta, d is the number of parameters\n",
    "    theta = np.zeros((d,1))\n",
    "    # retrieve discount factor\n",
    "    gamma = sampler.gamma\n",
    "    \n",
    "    for i in range(num_epi):\n",
    "        s_list, a_list, r_list = generate_episode(sampler, polf, theta, num_steps)\n",
    "        for t in range(num_steps+1):\n",
    "            G = np.dot(np.power(gamma, np.arange(num_steps + 1 - t)), np.array(r_list[t:]))\n",
    "            theta += alpha*np.power(gamma,t)*polf_grad(s_list[t], a_list[t], theta)\n",
    "            \n",
    "    return theta\n",
    "\n",
    "def generate_episode(sampler: RL_interface_FA, polf: Callable[[S, np.ndarray], A], \n",
    "                     theta: np.ndarray, num_steps: int) \\\n",
    "        -> Tuple[List[S], List[A], List[float]]:\n",
    "    # generate one episode\n",
    "    s = sampler.init_state_gen()\n",
    "    s_list = [s]\n",
    "    a = polf(s, theta)\n",
    "    a_list = [a]\n",
    "    r_list = []\n",
    "    for t in range(num_steps):\n",
    "        sp, r = sampler.state_reward_func(s, a)\n",
    "        ap = polf(sp, theta)\n",
    "        \n",
    "        s_list.append(sp)\n",
    "        a_list.append(ap)\n",
    "        r_list.append(r)\n",
    "        s, a = sp, ap\n",
    "        \n",
    "    _, r = sampler.state_reward_func(s, a)\n",
    "    r_list.append(r)\n",
    "    \n",
    "    return s_list, a_list, r_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic Policy Gradient Algorithms with TD(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic_td_error(sampler: RL_interface_FA, polf: Callable[[S, np.ndarray], A], \n",
    "              log_polf_grad: Callable[[S, A, np.ndarray], np.ndarray], \n",
    "              value_func: Callable[[S, np.ndarray], float],\n",
    "              value_func_grad: Callable[[S, np.ndarray], np.ndarray],\n",
    "              m: int, n: int, num_epi: int, num_steps: int, alpha_v: float, alpha_th: float) \\\n",
    "            -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # implementation of actor-critic td error algorithm w/o eligibility traces, i.e. TD(0)\n",
    "    # initialize parameters\n",
    "    theta = np.zeros((m,1))\n",
    "    v = np.zeros((n,1))\n",
    "    \n",
    "    gamma = sampler.gamma\n",
    "    for i in range(num_epi):\n",
    "        # generate initial state, and initialize P\n",
    "        s = sampler.init_state_gen()\n",
    "        P = 1\n",
    "        for t in range(num_steps):\n",
    "            # take action given by policy, and observe new state and reward\n",
    "            a = polf(s, theta)\n",
    "            sp, r = sampler.state_reward_func(s, a)\n",
    "            # find the td error and update parameters\n",
    "            delta = r + gamma*value_func(sp, v) - value_func(s, v)\n",
    "            v += alpha_v * delta * value_func_grad(s, v)\n",
    "            theta += alpha_th * P * delta * log_polf_grad(s, a, theta)\n",
    "            # update P and set current state to next state\n",
    "            P *= gamma\n",
    "            s = sp\n",
    "            \n",
    "    return v, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic Policy Gradient Algorithms with Eligiblity-Traces-based TD(Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic_eligibility_traces(sampler: RL_interface_FA, polf: Callable[[S, np.ndarray], A], \n",
    "              log_polf_grad: Callable[[S, A, np.ndarray], np.ndarray], \n",
    "              value_func: Callable[[S, np.ndarray], float],\n",
    "              value_func_grad: Callable[[S, np.ndarray], np.ndarray],\n",
    "              lambda_v: float, lambda_th: float,\n",
    "              m: int, n: int, num_epi: int, num_steps: int, alpha_v: float, alpha_th: float) \\\n",
    "            -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # implementation of actor-critic algorithm w/ eligibility traces, i.e. TD(lambda)\n",
    "    # initialize parameters\n",
    "    theta = np.zeros((m,1))\n",
    "    v = np.zeros((n,1))\n",
    "    \n",
    "    gamma = sampler.gamma\n",
    "    for i in range(num_epi):\n",
    "        # generate initial state, and initialize P\n",
    "        s = sampler.init_state_gen()\n",
    "        P = 1\n",
    "        # initialize eligibility traces vectors\n",
    "        z_v = np.zeros((n, 1))\n",
    "        z_th = np.zeros((m, 1))\n",
    "        for t in range(num_steps):\n",
    "            # take action given by policy, and observe new state and reward\n",
    "            a = polf(s, theta)\n",
    "            sp, r = sampler.state_reward_func(s, a)\n",
    "            # find the td error\n",
    "            delta = r + gamma*value_func(sp, v) - value_func(s, v)\n",
    "            # update eligivility traces\n",
    "            z_v = gamma * lambda_v * z_v + value_func_grad(s, v)\n",
    "            z_th = gamma * lambda_th * z_th + P * log_polf_grad(s, a, theta)\n",
    "            # update parameters\n",
    "            v += alpha_v * delta * z_v\n",
    "            theta += alpha_th * P * delta * z_th\n",
    "            # update P and set current state to next state\n",
    "            P *= gamma\n",
    "            s = sp\n",
    "            \n",
    "    return v, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of Compatible Function Approximation Theorem\n",
    "__Theorem:__\n",
    "\n",
    "__1.__ Critic gradient is compatible with the Actor score function\n",
    "\n",
    "$$\n",
    "\\nabla_w Q(s,a;w) = \\nabla_\\theta \\log\\pi(s,a;\\theta)\n",
    "$$\n",
    "\n",
    "__2.__ Critic parameters $w$ minimize the following mean-squared error:\n",
    "\n",
    "$$\n",
    "\\epsilon = \\int_{\\mathcal S}\\rho^\\pi(s)\\int_{\\mathcal A} \\pi(s,a;\\theta)\\big(Q^\\pi(s,a)-Q(s,a;w)\\big)^2\\cdot da\\cdot ds\n",
    "$$\n",
    "\n",
    "Then the Policy Gradient using critic Q(s,a;w) is exact:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int_{\\mathcal S}\\rho^\\pi(s)\\int_{\\mathcal A}\\nabla_\\theta \\pi(s,a;\\theta)\\cdot Q(s,a;w)\\cdot da\\cdot ds\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Proof:__\n",
    "\n",
    "For $w$ that minimizes $\\epsilon$ we have that\n",
    "\n",
    "$$\n",
    "\\int_{\\mathcal S}\\rho^\\pi(s)\\int_{\\mathcal A} \\pi(s,a;\\theta)\\big(Q^\\pi(s,a)-Q(s,a;w)\\big)\\nabla_w Q(s,a;w)\\cdot da\\cdot ds = 0\n",
    "$$\n",
    "\n",
    "Since $\\nabla_w Q(s,a;w) = \\nabla_\\theta \\log \\pi(s,a;\\theta)$ we get\n",
    "\n",
    "$$\n",
    "\\int_{\\mathcal S}\\rho^\\pi(s)\\int_{\\mathcal A} \\pi(s,a;\\theta)\\big(Q^\\pi(s,a)-Q(s,a;w)\\big)\\nabla_\\theta \\log \\pi(s,a;\\theta)\\cdot da\\cdot ds = 0\n",
    "$$\n",
    "\n",
    "Which then gives\n",
    "\n",
    "$$\n",
    "\\int_{\\mathcal S}\\rho^\\pi(s)\\int_{\\mathcal A} \\pi(s,a;\\theta)\\cdot Q^\\pi(s,a)\\nabla_\\theta \\log \\pi(s,a;\\theta)\\cdot da\\cdot ds = \\int_{\\mathcal S}\\rho^\\pi(s)\\int_{\\mathcal A} \\pi(s,a;\\theta)\\cdot Q(s,a;w)\\nabla_\\theta \\log \\pi(s,a;\\theta)\\cdot da\\cdot ds \n",
    "$$\n",
    "\n",
    "But \n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int_{\\mathcal S}\\rho^\\pi(s)\\int_{\\mathcal A} \\pi(s,a;\\theta)\\cdot Q^\\pi(s,a)\\nabla_\\theta \\log \\pi(s,a;\\theta)\\cdot da\\cdot ds \\\\\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta) & = \\int_{\\mathcal S}\\rho^\\pi(s)\\int_{\\mathcal A} \\pi(s,a;\\theta)\\cdot Q(s,a;w)\\nabla_\\theta \\log \\pi(s,a;\\theta)\\cdot da\\cdot ds \\\\\n",
    "& = \\int_{\\mathcal S}\\rho^\\pi(s)\\int_{\\mathcal A} \\nabla_\\theta \\pi(s,a;\\theta)\\cdot Q(s,a;w)\\cdot da\\cdot ds\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
