{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-up and code for Assignment 6 - Week 3\n",
    "\n",
    "### To do\n",
    "- Model Merton's Portfolio problem as an MDP (write the model in LaTeX)\n",
    "- Implement this MDP model in code\n",
    "- Try recovering the closed-form solution with a DP algorithm that you implemented previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merton's Portfolio Problem\n",
    "### Informal Problem Statement\n",
    "- You will live for $T$ more years, where $T$ is deterministic. \n",
    "- $W_0 > 0 $ is the current wealth $+$ the present value of future income minus debts. \n",
    "- You can invest in $n$ risky assets and 1 riskless asset. \n",
    "- Each asset has a known normal distribution of returns. You are allowed to go long or short any fractional quantities of assets. \n",
    "- Trading is done in continuous time $0 \\leq t < T$, with no transaction costs.\n",
    "- You can consume any fractional amount of wealth at any time.\n",
    "- Dynamic Decision: Optimal Allocation and Consumption at each time with the goal of maximizing the lifetime-aggregated utility of consumption.\n",
    "- Consumtion utility assumed to have Constant Relative Risk-Aversion (CRRA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Notation\n",
    "- The riskless asset is described by: $dR_t = r\\cdot R_t \\cdot dt$\n",
    "- Risky asset $i$ is governed by: $dS_{i,t} = \\mu_i\\cdot S_{i,t}\\cdot dt + \\sigma_i\\cdot S_{i,t} \\cdot dz_{t}$, if we consider $\\mu$ and $\\sigma$ to be vectors and matrices we can write the Geometric Brownian motion in vector form as $dS_{t} = \\mu_i\\cdot S_{t}\\cdot dt + \\sigma\\cdot S_{t} \\cdot dz_{t}$. Where $S_t$ and $dS_t$ are vectors.\n",
    "- $\\mu > r> 0,~ \\sigma >0$ for all $n$ assets.\n",
    "- The wealth at time $t$ is denoted by $W_t > 0$.\n",
    "- Fraction of wealth allocated to risky asset $i$ is denoted by $\\pi_i(t,W_t)$.\n",
    "- Fraction of wealth allocated to riskless asset is denoted by $1 - \\sum_{i=1}^n \\pi_i$.\n",
    "- Wealth consumption denoted by $c(t,W(t)) \\geq 0$\n",
    "- Utility of consumption function $U(x) = \\frac{x^{1-\\gamma}}{1-\\gamma}$ for $0<\\gamma\\neq1$\n",
    "- Utility of consumption function $U(x) = \\log(x)$ for $\\gamma =1$\n",
    "- $\\gamma=$ (constant) Relative Risk-Aversion $\\frac{-x\\cdot U''(x)}{U'(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process Model\n",
    "We can leverage the framework of an MDP to model Merton's Portolio Problem and thereby use Dynamic Programming to find solutions (more efficient techniques will be covered later). We model this problem using a single risky asset and one riskless asset.\n",
    "- The _State_ is $(t,W_t)$\n",
    "- The _Action_ is $[\\pi_t, c_t]$\n",
    "- The _Reward_ per unit time is $U(c_t)$\n",
    "- The _Return_ is the usual accumulated discounted _Reward_\n",
    "- The goal is to find a _Policy_: $(t, W_t) \\rightarrow [\\pi_t, c_t]$ that maximizes the _Expected Return_\n",
    "- Note: $c_t \\geq 0$, but $\\pi_t$ is unconstrained\n",
    "- The _Transitions_ are governed by the processes mentioned previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for modeling Merton's Portfolio Problem as MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import numpy as np\n",
    "\n",
    "class MertonPortfolio(NamedTuple):\n",
    "    T: float\n",
    "    rho: float\n",
    "    r: float\n",
    "    mu: np.ndarray \n",
    "    cov: np.ndarray\n",
    "    gamma: float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have 10 days of trading and we are only allowed to adjust once every trading day. Assume that the wealth can take 100 values between 0 and 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "States = set()\n",
    "for i in range(10):\n",
    "    for w in range(100):\n",
    "        States.add((i, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Any, Dict, Tuple, Set, Union\n",
    "import numpy as np\n",
    "\n",
    "class MP(NamedTuple):\n",
    "    States: Set[S]\n",
    "    P: Dict[S, Dict[S, float]]\n",
    "        \n",
    "        \n",
    "class MRP(NamedTuple):\n",
    "    mp: MP\n",
    "    R: Union[Dict[S, float], Dict[S, Dict[S, float]]]\n",
    "    gamma: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP(NamedTuple):\n",
    "    States: Set[S]\n",
    "    # the transitions depend on s, a, and s'\n",
    "    # mapping from a state to a mapping of an action to a mapping of a state to a float (probability)\n",
    "    P: Dict[S, Dict[A, Dict[S, float]]]\n",
    "    Actions: A\n",
    "    # reward is a function of the current state,  and the action\n",
    "    R: Union[Dict[S, Dict[A, float]], Dict[S, Dict[A, Dict[S, float]]]]\n",
    "    gamma: float\n",
    "\n",
    "        \n",
    "class Policy(NamedTuple):\n",
    "    # state to action to a probability\n",
    "    pi: Dict[S, Dict[A, float]]\n",
    "        \n",
    "\n",
    "class state_value_function(NamedTuple):\n",
    "    vf: Dict[S,  float]\n",
    "        \n",
    "        \n",
    "class action_value_function(NamedTuple):\n",
    "    vf: Dict[S, Dict[A, float]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
