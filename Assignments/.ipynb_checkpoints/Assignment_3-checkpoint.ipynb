{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-up and code for Assignment 3 - Week 2\n",
    "\n",
    "### To do:\n",
    "- ~~Write the Bellman equation for MRP Value Function and code to calculate MRP Value Function (based on Matrix inversion method you learnt in this lecture)~~\n",
    "- ~~Write out the MDP definition, Policy definition and MDP Value Function definition (in LaTeX) in your own style/notation (so you really internalize these concepts)~~\n",
    "- ~~Think about the data structure/class design (in Python 3) to represent MDP, Policy, Value Function, and implement them with clear type definitions~~\n",
    "- ~~The data struucture/code design of MDP should be incremental (and not independent) to that of MRP~~\n",
    "- ~~Separately implement the r(s,s',a) and R(s,a) = \\sum_{s'} p(s,s',a) * r(s,s',a) definitions of MDP~~\n",
    "- ~~Write code to convert/cast the r(s,s',a) definition of MDP to the R(s,a) definition of MDP (put some thought into code design here)~~\n",
    "- ~~Write code to create a MRP given a MDP and a Policy~~\n",
    "- Write out all 8 MDP Bellman Equations and also the transformation from Optimal Action-Value function to Optimal Policy (in LaTeX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equation for MRP\n",
    "The value f being in the current state $s$ in an MRP can be decomposed into two parts: the immediate reward $R_{t+1}$, and the discounted value of being in the successor state $\\gamma v(S_{t+1})$. Hence, we can define the value function for state $s$ as:\n",
    "<br>\n",
    "<br>\n",
    "$$v(s) = E[R_{t+1} + \\gamma v(S_{t+1}) ~|~ S_t = s]$$\n",
    "<br>\n",
    "<br>\n",
    "Which then for a discrete state space translates to:\n",
    "<br>\n",
    "<br>\n",
    "$$v(s) = \\mathcal R_s + \\gamma \\sum_{s'\\in\\mathcal S} \\mathcal P_{ss'} v(s')$$\n",
    "<br>\n",
    "<br>\n",
    "Using matrix notation this can be written as:\n",
    "<br>\n",
    "<br>\n",
    "$$v = \\mathcal R + \\gamma \\mathcal P v$$\n",
    "<br>\n",
    "<br>\n",
    "We can then find $v$ by using matrix operations as follows:\n",
    "<br>\n",
    "<br>\n",
    "$$v = (I - \\gamma \\mathcal P)^{-1} \\mathcal R$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_function(mrp: MRP) -> Dict[any, float]:\n",
    "    R, P = get_matrices(mrp)\n",
    "    I = np.identity(r.shape[0])\n",
    "    \n",
    "    v = np.matmul(np.inv(np.subtract(I, mrp.gamma*P)), R)\n",
    "    return v\n",
    "\n",
    "\n",
    "def get_matrices(mrp: MRP) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # function to convert reward and transition dicts to matrices\n",
    "    # this is necessary to solve the value funciton using matrix operations\n",
    "    # assumes rewards are defined as a funciton of the current state\n",
    "    sz = len(mrp.S)\n",
    "    R = np.zeros((sz,1))\n",
    "    P = np.zeros((sz,sz))\n",
    "    \n",
    "    for i, s in enumerate(mrp.S):\n",
    "        R[i] = mrp.R[s]\n",
    "        for j, sp in enumerate(mrp.S):\n",
    "            if sp in mrp.P[s].keys():\n",
    "                P[i, j] = mrp.P[s][sp]\n",
    "                \n",
    "    return R, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Decision Process\n",
    "A Markov Decision Process (MDP) consists of a finite set of states $\\mathcal S$, a transition probability matrix $\\mathcal P$ (that now also depends on the action taken), a reward function $\\mathcal R$, a discount factor $\\gamma$, and a finite set of actions $\\mathcal A$. An MDP is then defined by the parameters $\\mathcal S$, $\\mathcal A$, $\\mathcal P$, $\\mathcal R$ and $\\gamma$. An MDP is thus an extension of an MRP.\n",
    "\n",
    "\n",
    "#### Policy\n",
    "A policy $\\pi$ is a probabilistic distribution over actions $a \\in \\mathcal A$ given a state $s$.\n",
    "<br>\n",
    "<br>\n",
    "$$\\pi(a~|~s) = \\mathbb P[A_t = a ~|~ S_t = s]$$\n",
    "<br>\n",
    "- A policy fully defines the behaviour of an agent.\n",
    "- An MDP combined with a policy results in an MRP.\n",
    "\n",
    "\n",
    "#### MDP Value Function\n",
    "We can define the value function for an MDP in two ways, firstly as solely a function of the current state:\n",
    "<br>\n",
    "- The state-value function $v_\\pi(s)$ is the expected aggregated and discounted reward starting in state $s$ and then following policy $\\pi$.\n",
    "<br>\n",
    "<br>\n",
    "$$v_\\pi(s) = \\mathbb E_\\pi[ G_t ~|~ S_t = s]$$\n",
    "<br>\n",
    "and secondly as a function of both the current state and the action (action-value function):\n",
    "<br>\n",
    "- The action-value function $q_\\pi(s,a)$ is the expected aggregated and discounted reward starting in state $s$, taking action $a$ and then following policy $\\pi$.\n",
    "<br>\n",
    "<br>\n",
    "$$q_\\pi(s,a) = \\mathbb E_\\pi[ G_t ~|~ S_t = s, A_t = a]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP(NamedTuple):\n",
    "    States: Set[S]\n",
    "    # the transitions depend on s, a, and s'\n",
    "    # mapping from a state to a mapping of an action to a mapping of a state to a float (probability)\n",
    "    P: Dict[S, Dict[A, Dict[S, float]]]\n",
    "    Actions: A\n",
    "    # reward is a function of the current state,  and the action\n",
    "    R: Dict[S, Dict[A, float]]\n",
    "    gamma: float\n",
    "        \n",
    "        \n",
    "class MDP(NamedTuple):\n",
    "    States: Set[S]\n",
    "    # the transitions depend on s, a, and s'\n",
    "    # mapping from a state to a mapping of an action to a mapping of a state to a float (probability)\n",
    "    P: Dict[S, Dict[A, Dict[S, float]]]\n",
    "    Actions: A\n",
    "    # reward is a function of the current state s, the action a, and the next state sp\n",
    "    R: Dict[S, Dict[A, Dict[S, float]]]\n",
    "    gamma: float\n",
    "        \n",
    "        \n",
    "class Policy(NamedTuple):\n",
    "    # state to action to a probability\n",
    "    pi: Dict[S, Dict[A, float]]\n",
    "        \n",
    "\n",
    "class value_function(NamedTuple):\n",
    "    vf: Dict[S, Dict[A, float]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the code above we can define the reward in multiple ways. The reward can be a function of the current state and the action taken: $R(s,a)$.\n",
    "The reward can also be a function of the current state, the action taken, and the next state: $r(s,s',a)$. The code below shows how we can convert the latter definition to the former using the laws of expectation:\n",
    "$$R(s,a) = \\sum_{s'\\in \\mathcal S}\\mathcal P_{s,s',a}r(s,s',a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_reward_mdp(mdp: MDP) -> Dict[S, Dict[A, float]]:\n",
    "    # function to convert an mdp using r(s, s', a) to R(s,a)\n",
    "    new_r = {}\n",
    "    for s in mdp.States:\n",
    "        new_r[s] = {}\n",
    "        for a in mdp.R[s]:\n",
    "            new_r[s][a] = 0\n",
    "            for sp in mdp.R[s][a]:\n",
    "                new_r[s][a] += mdp.R[s][a][sp]*mdp.P[s][a][sp]\n",
    "    \n",
    "    return new_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a policy and an MDP we can create an MRP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdp_to_mrp(mdp: MDP, policy: Policy) -> MRP:\n",
    "    # function to convert an MDP and a policy into an MRP\n",
    "    States = mdp.States\n",
    "    gamma = mdp.gamma\n",
    "    reward = {}\n",
    "    probs = {}\n",
    "    for s in States:\n",
    "        # assume the reward is just a function of the current state R(s)\n",
    "        reward[s] = 0\n",
    "        probs[s] = {}\n",
    "        for a in policy[s]:\n",
    "            reward[s] += policy[s][a]*mdp.R[s][a]\n",
    "            for sp in mdp.P[s][a]:\n",
    "                if sp not in probs[s]:\n",
    "                    probs[s][sp] = 0\n",
    "                    reward[s][sp]\n",
    "                probs[s][sp] += policy[s][a]*mdp.P[s][a][sp]\n",
    "    \n",
    "    mp = MP(States, probs)\n",
    "    mrp = MRP(mp, reward, gamma)\n",
    "    \n",
    "    return mrp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equations\n",
    "As we saw earlier the state-value function $v_\\pi(s)$ can be expressed as the immediate and future discounted rewards starting in state $s$. We can therefore decompose the state-value function as follows:\n",
    "$$v_\\pi(s) = \\mathbb E[R_{t+1} + \\gamma v_\\pi(S_{t+1})~|~S_t = s]$$\n",
    "<br>\n",
    "Similarly, we can decompose the action-value function into the same two parts:\n",
    "<br> <br>\n",
    "$$q_\\pi(s,a) = \\mathbb E[R_{t+1} + \\gamma q_\\pi(S_{t+1},A_{t+1})~|~S_t = s, A_{t} = a]$$\n",
    "<br> <br>\n",
    "We can see how the state-value function is connected to the action-value function, and how we can transform the latter to the former using the policy $\\pi$:\n",
    "<br> <br>\n",
    "$$v_\\pi(s) = \\sum_{a \\in A}\\pi(a~|~s) q_\\pi(s,a)$$\n",
    "<br> <br>\n",
    "The action-value function also depends on the state-value function. If we take action $a$ in state $s$ the value is the immediate reward, plus the discounted value of the state we end up in when taking action $a$. This can be illustrated by the following equation:\n",
    "<br> <br>\n",
    "$$q_\\pi(s,a) = \\mathcal R_s^a + \\gamma\\sum_{s' \\in S}\\mathcal P_{s,s'}^a v_\\pi(s')$$\n",
    "<br> <br>\n",
    "Combining the last two equations gives us:\n",
    "<br> <br>\n",
    "$$v_\\pi(s) = \\sum_{a \\in A}\\pi(a~|~s)\\Bigg( \\mathcal R_s^a + \\gamma\\sum_{s' \\in S}\\mathcal P_{s,s'}^a v_\\pi(s')\\Bigg)$$\n",
    "<br> <br>\n",
    "We can also combine them into:\n",
    "<br> <br>\n",
    "$$q_\\pi(s,a) = \\mathcal R_s^a + \\gamma\\sum_{s' \\in S}\\mathcal P_{s,s'}^a \\sum_{a' \\in A}\\pi(a'~|~s') q_\\pi(s',a')$$\n",
    "<br> <br>\n",
    "Finally, using matrix notation we can write the state-value function as:\n",
    "<br> <br>\n",
    "$$v_\\pi(s) = \\mathcal R_\\pi + \\gamma\\mathcal P_\\pi v_\\pi$$\n",
    "<br> <br>\n",
    "which can then be transformed into:\n",
    "<br> <br>\n",
    "$$v_\\pi(s) = (I - \\gamma\\mathcal P_\\pi)^{-1}\\mathcal R_\\pi$$\n",
    "<br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Value and Optimal Policy\n",
    "The optimal action-value function is defined as:\n",
    "<br><br>\n",
    "$$q_*(s,a) = max_\\pi q_\\pi(s,a)$$\n",
    "<br><br>\n",
    "We say that $\\pi \\geq \\pi'$ if $v_\\pi(s) \\geq v_{\\pi'}(s)$ for all states $s$. Then,\n",
    "- For any MDP there exist an optimal policy $\\pi_*$ that is better than or equal to all other policies, $\\pi_* \\geq \\pi, \\forall \\pi$.\n",
    "- All optimal policies achieve the optimal state-value function, $v_{\\pi_*}(s) = v_*(s), \\forall s$.\n",
    "- All optimal policies achieve the optimal action-value function, $q_{\\pi_*}(s,a) = q_*(s,a), \\forall s, a$.\n",
    "\n",
    "\n",
    "An optimal policy can be found by maximizing over $q_*(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "Here is code from previous assignments that are necessary to run the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Any, Dict, Tuple, Set\n",
    "import numpy as np\n",
    "\n",
    "class MP(NamedTuple):\n",
    "    States: Set[S]\n",
    "    P: Dict[S, Dict[S, float]]\n",
    "        \n",
    "        \n",
    "class MRP(NamedTuple):\n",
    "    # assumes that the reward is just a function of the current state\n",
    "    mp: MP\n",
    "    R: Dict[S, float]\n",
    "    gamma: float\n",
    "        \n",
    "        \n",
    "class MRP(NamedTuple):\n",
    "    # assumes that the reward is a function of the transition\n",
    "    mp: MP\n",
    "    R: Dict[S, Dict[S, float]]\n",
    "    gamma: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
