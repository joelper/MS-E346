{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-up and code for Feb 6\n",
    "\n",
    "### To Do\n",
    "- ~~Write code for the interface for tabular RL algorithms. The core of this interface should be a mapping from a (state, action) pair to a sampling of the (next state, reward) pair. It is important that this interface doesn't present the state-transition probability model or the reward model.~~\n",
    "- ~~Implement any tabular Monte-Carlo algorithm for Value Function prediction~~\n",
    "- ~~Implement tabular 1-step TD algorithm for Value Function prediction~~\n",
    "- ~~Test the above implementation of Monte-Carlo and TD Value Function prediction algorithms versus DP Policy Evaluation algorithm on an example MDP~~\n",
    "- Prove that fixed learning rate (step size alpha) for MC is equivalent to an exponentially decaying average of episode returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface for tabular RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.MDP import MDP, Policy, V, Q\n",
    "from modules.state_action_vars import S, A\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def RL_interface(mdp: MDP, s: S, a: A) -> Tuple[S, float]:\n",
    "    # interface that takes in a state 's' and an action 'a', and returns a new state 'sp' and an observed reward 'r'\n",
    "    sp = sp_sampler(mdp, s, a)\n",
    "    # check how the reward is defined\n",
    "    if type(mdp.R[s][a]) == float:\n",
    "        r = mdp.R[s][a]\n",
    "    else:\n",
    "        r = mdp.R[s][a][sp]\n",
    "        \n",
    "    return sp, r\n",
    "\n",
    "\n",
    "def sp_sampler(mdp: MDP, s: S, a: A) -> S:\n",
    "    # function that takes in an MDP, a state and an action and samples a new state sp from that distribution\n",
    "    p_cum = 0\n",
    "    prob = np.random.rand()\n",
    "    for sp in mdp.P[s][a].keys():\n",
    "        p_cum += mdp.P[s][a][sp]\n",
    "        if prob <= p_cum:\n",
    "            return sp\n",
    "        \n",
    "    return sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Algorithm for Value Function Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(policy: Policy, mdp: MDP, num_epi: int, num_steps: int) -> V:\n",
    "    # follows the first visit MC algorithm outlined in Sutton's RL book\n",
    "    v = {}\n",
    "    returns = {}\n",
    "    gamma = mdp.gamma\n",
    "    for s in mdp.States:\n",
    "        v[s] = 0\n",
    "        returns[s] = []\n",
    "        \n",
    "    for i in range(num_epi):\n",
    "        # generate an episode\n",
    "        s_list, a_list, r_list = generate_path(policy, mdp, num_steps)\n",
    "        # initialize the episode return\n",
    "        G = 0\n",
    "        for j in range(num_steps-1, 0, -1):\n",
    "            G = gamma*G + r_list[j]\n",
    "            if s_list[j] not in s_list[:j]:\n",
    "                returns[s_list[j]].append(G)\n",
    "                \n",
    "    for s in mdp.States:\n",
    "        v[s] = np.mean(returns[s])  \n",
    "        \n",
    "    return v\n",
    "\n",
    "\n",
    "def generate_path(policy: Policy, mdp: MDP, num_steps: int) -> Tuple[list, list, list]:\n",
    "    # generate a sample path that follows the provided policy\n",
    "    # the function returns: S_0, A_0, R_1, ... , S_(T-1), A_(T-1), R_T\n",
    "    s_list = []\n",
    "    a_list = []\n",
    "    r_list = []\n",
    "    \n",
    "    s0 = random.sample(mdp.States,1).pop()\n",
    "    s_list.append(s0)\n",
    "    a0 = action_sampler(policy, s0)\n",
    "    a_list.append(a0)\n",
    "    for i in range(num_steps - 1):\n",
    "        sp, r = RL_interface(mdp, s_list[-1], a_list[-1])\n",
    "        a = action_sampler(policy, sp)\n",
    "        s_list.append(sp)\n",
    "        a_list.append(a)\n",
    "        r_list.append(r)\n",
    "        \n",
    "    # sample the last reward\n",
    "    _, r = RL_interface(mdp, s_list[-1], a_list[-1])\n",
    "    r_list.append(r)\n",
    "    \n",
    "    return s_list, a_list, r_list\n",
    "\n",
    "\n",
    "def action_sampler(policy: Policy, s: S) -> A:\n",
    "    # function that takes in a policy and a state and samples an action according to this policy \n",
    "    p_cum = 0\n",
    "    prob = np.random.rand()\n",
    "    for a in policy[s].keys():\n",
    "        p_cum += policy[s][a]\n",
    "        if prob <= p_cum:\n",
    "            return a\n",
    "        \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-step TD Algorithm for Value Function Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_0(policy: Policy, mdp: MDP, alpha: float, fixed_alpha: bool, num_epi: int, num_steps: int) -> V:\n",
    "    # follows the TD(0) algorithm as it is outlined in the Sutton RL book\n",
    "    v = {}\n",
    "\n",
    "    for s in mdp.States:\n",
    "        v[s] = 0\n",
    "        \n",
    "    if not fixed_alpha:\n",
    "        alpha = 1.0\n",
    "    \n",
    "    for i in range(num_epi):\n",
    "        s = random.sample(mdp.States,1).pop()\n",
    "        \n",
    "        for j in range(num_steps):\n",
    "            a = action_sampler(policy, s)\n",
    "            sp, r = RL_interface(mdp, s, a)\n",
    "            v[s] += alpha*(r + mdp.gamma*v[sp] - v[s])\n",
    "            s = sp\n",
    "        \n",
    "        if not fixed_alpha:\n",
    "            alpha = 1.0/(i + 2)\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for Comparing MC, TD(0) and DP Algorithms\n",
    "We will reuse the example from Assignment 4, which is a simple gridworld problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.gridworld import gridworld\n",
    "gw = gridworld(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a policy that is always moving up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {}\n",
    "for s in gw.States:\n",
    "    policy[s] = {}\n",
    "    for a in gw.Actions:\n",
    "        if a == 2:\n",
    "            policy[s][a] = 1.0\n",
    "        else:\n",
    "            policy[s][a] = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.DP import policy_eval, policy_iter\n",
    "policy = policy_iter(gw, policy, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, evaluate the value function using policy evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_pe = policy_eval(gw, policy, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the policy using First Visit Monte-Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_mc = first_visit_mc(policy, gw, 10000, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, predict the value function using One-Step Temporal Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_td = TD_0(policy, gw, 1/100, True, 10000, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the three methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) Policy Evaluation: 30.000 \tMonte-Carlo: 29.999 \tTD(0): 30.000\n",
      "(0, 1) Policy Evaluation: 24.916 \tMonte-Carlo: 24.953 \tTD(0): 19.973\n",
      "(0, 2) Policy Evaluation: 20.730 \tMonte-Carlo: 20.933 \tTD(0): 10.652\n",
      "(0, 3) Policy Evaluation: 18.202 \tMonte-Carlo: 18.205 \tTD(0): 3.336\n",
      "(1, 0) Policy Evaluation: 24.940 \tMonte-Carlo: 24.979 \tTD(0): 18.687\n",
      "(1, 1) Policy Evaluation: 21.197 \tMonte-Carlo: 21.111 \tTD(0): 6.456\n",
      "(1, 2) Policy Evaluation: 16.993 \tMonte-Carlo: 17.158 \tTD(0): 2.573\n",
      "(1, 3) Policy Evaluation: 20.730 \tMonte-Carlo: 21.129 \tTD(0): 4.196\n",
      "(2, 0) Policy Evaluation: 20.970 \tMonte-Carlo: 21.517 \tTD(0): 5.591\n",
      "(2, 1) Policy Evaluation: 19.036 \tMonte-Carlo: 19.045 \tTD(0): 4.331\n",
      "(2, 2) Policy Evaluation: 21.197 \tMonte-Carlo: 21.226 \tTD(0): 13.171\n",
      "(2, 3) Policy Evaluation: 24.916 \tMonte-Carlo: 25.040 \tTD(0): 16.626\n",
      "(3, 0) Policy Evaluation: 18.412 \tMonte-Carlo: 18.406 \tTD(0): 2.928\n",
      "(3, 1) Policy Evaluation: 20.970 \tMonte-Carlo: 20.934 \tTD(0): 11.443\n",
      "(3, 2) Policy Evaluation: 24.940 \tMonte-Carlo: 24.955 \tTD(0): 22.421\n",
      "(3, 3) Policy Evaluation: 30.000 \tMonte-Carlo: 29.999 \tTD(0): 30.000\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(vf_td.keys()):\n",
    "    print(key, 'Policy Evaluation: {:0.3f} \\tMonte-Carlo: {:0.3f} \\tTD(0): {:0.3f}'.format(vf_pe[key], vf_mc[key], vf_td[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof that a Fixed Learning Rate For MC is Equivalent to Exponentially Decaying Average of Episode Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
