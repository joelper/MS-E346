{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-up and code for Feb 6\n",
    "\n",
    "### To Do\n",
    "- ~~Write code for the interface for tabular RL algorithms. The core of this interface should be a mapping from a (state, action) pair to a sampling of the (next state, reward) pair. It is important that this interface doesn't present the state-transition probability model or the reward model.~~\n",
    "- ~~Implement any tabular Monte-Carlo algorithm for Value Function prediction~~\n",
    "- ~~Implement tabular 1-step TD algorithm for Value Function prediction~~\n",
    "- ~~Test the above implementation of Monte-Carlo and TD Value Function prediction algorithms versus DP Policy Evaluation algorithm on an example MDP~~\n",
    "- Prove that fixed learning rate (step size alpha) for MC is equivalent to an exponentially decaying average of episode returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface for tabular RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.MDP import MDP, Policy, V, Q\n",
    "from modules.state_action_vars import S, A\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def RL_interface(mdp: MDP, s: S, a: A) -> Tuple[S, float]:\n",
    "    # interface that takes in a state 's' and an action 'a', and returns a new state 'sp' and an observed reward 'r'\n",
    "    sp = sp_sampler(mdp, s, a)\n",
    "    # check how the reward is defined\n",
    "    if type(mdp.R[s][a]) == float:\n",
    "        r = mdp.R[s][a]\n",
    "    else:\n",
    "        r = mdp.R[s][a][sp]\n",
    "        \n",
    "    return sp, r\n",
    "\n",
    "\n",
    "def sp_sampler(mdp: MDP, s: S, a: A) -> S:\n",
    "    # function that takes in an MDP, a state and an action and samples a new state sp from that distribution\n",
    "    p_cum = 0\n",
    "    prob = np.random.rand()\n",
    "    for sp in mdp.P[s][a].keys():\n",
    "        p_cum += mdp.P[s][a][sp]\n",
    "        if prob <= p_cum:\n",
    "            return sp\n",
    "        \n",
    "    return sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Algorithm for Value Function Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(policy: Policy, mdp: MDP, num_epi: int, num_steps: int) -> V:\n",
    "    # follows the first visit MC algorithm outlined in Sutton's RL book\n",
    "    v = {}\n",
    "    returns = {}\n",
    "    gamma = mdp.gamma\n",
    "    for s in mdp.States:\n",
    "        v[s] = 0\n",
    "        returns[s] = []\n",
    "        \n",
    "    for i in range(num_epi):\n",
    "        # generate an episode\n",
    "        s_list, a_list, r_list = generate_path(policy, mdp, num_steps)\n",
    "        # initialize the episode return\n",
    "        G = 0\n",
    "        for j in range(num_steps-1, 0, -1):\n",
    "            G = gamma*G + r_list[j]\n",
    "            if s_list[j] not in s_list[:j]:\n",
    "                returns[s_list[j]].append(G)\n",
    "                \n",
    "    for s in mdp.States:\n",
    "        v[s] = np.mean(returns[s])  \n",
    "        \n",
    "    return v\n",
    "\n",
    "\n",
    "def generate_path(policy: Policy, mdp: MDP, num_steps: int) -> Tuple[list, list, list]:\n",
    "    # generate a sample path that follows the provided policy\n",
    "    # the function returns: S_0, A_0, R_1, ... , S_(T-1), A_(T-1), R_T\n",
    "    s_list = []\n",
    "    a_list = []\n",
    "    r_list = []\n",
    "    \n",
    "    s0 = random.sample(mdp.States,1).pop()\n",
    "    s_list.append(s0)\n",
    "    a0 = action_sampler(policy, s0)\n",
    "    a_list.append(a0)\n",
    "    for i in range(num_steps - 1):\n",
    "        sp, r = RL_interface(mdp, s_list[-1], a_list[-1])\n",
    "        a = action_sampler(policy, sp)\n",
    "        s_list.append(sp)\n",
    "        a_list.append(a)\n",
    "        r_list.append(r)\n",
    "        \n",
    "    # sample the last reward\n",
    "    _, r = RL_interface(mdp, s_list[-1], a_list[-1])\n",
    "    r_list.append(r)\n",
    "    \n",
    "    return s_list, a_list, r_list\n",
    "\n",
    "\n",
    "def action_sampler(policy: Policy, s: S) -> A:\n",
    "    # function that takes in a policy and a state and samples an action according to this policy \n",
    "    p_cum = 0\n",
    "    prob = np.random.rand()\n",
    "    for a in policy[s].keys():\n",
    "        p_cum += policy[s][a]\n",
    "        if prob <= p_cum:\n",
    "            return a\n",
    "        \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-step TD Algorithm for Value Function Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_0(policy: Policy, mdp: MDP, alpha: float, num_epi: int, num_steps: int) -> V:\n",
    "    # follows the TD(0) algorithm as it is outlined in the Sutton RL book\n",
    "    v = {}\n",
    "    gamma = mdp.gamma\n",
    "    for s in mdp.States:\n",
    "        v[s] = 0\n",
    "    \n",
    "    for i in range(num_epi):\n",
    "        s = random.sample(mdp.States,1).pop()\n",
    "        \n",
    "        for j in range(num_steps):\n",
    "            a = action_sampler(policy, s)\n",
    "            sp, r = RL_interface(mdp, s, a)\n",
    "            v[s] += alpha*(r + gamma*v[sp] - v[s])\n",
    "            s = sp\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for Comparing MC, TD(0) and DP Algorithms\n",
    "We will reuse the example from Assignment 4, which is a simple gridworld problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.gridworld import gridworld\n",
    "gw = gridworld(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a policy that is always moving up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {}\n",
    "for s in gw.States:\n",
    "    policy[s] = {}\n",
    "    for a in gw.Actions:\n",
    "        if a == 2:\n",
    "            policy[s][a] = 1.0\n",
    "        else:\n",
    "            policy[s][a] = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, evaluate the value function using policy evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.DP import policy_eval\n",
    "vf_pe = policy_eval(gw, policy, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the policy using First Visit Monte-Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_mc = first_visit_mc(policy, gw, 1000, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, predict the value function using One-Step Temporal Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_td = TD_0(policy, gw, 0.1, 1000, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the three methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) Policy Evaluation: 29.999 \tMonte-Carlo: 29.588 \tTD(0): 30.000\n",
      "(0, 1) Policy Evaluation: 4.339 \tMonte-Carlo: 4.099 \tTD(0): 4.901\n",
      "(0, 2) Policy Evaluation: 1.631 \tMonte-Carlo: 1.511 \tTD(0): 1.114\n",
      "(0, 3) Policy Evaluation: 1.557 \tMonte-Carlo: 1.258 \tTD(0): 1.731\n",
      "(1, 0) Policy Evaluation: 5.458 \tMonte-Carlo: 5.871 \tTD(0): 5.894\n",
      "(1, 1) Policy Evaluation: 2.450 \tMonte-Carlo: 2.459 \tTD(0): 1.689\n",
      "(1, 2) Policy Evaluation: 1.254 \tMonte-Carlo: 0.735 \tTD(0): 0.548\n",
      "(1, 3) Policy Evaluation: 3.215 \tMonte-Carlo: 2.695 \tTD(0): 3.960\n",
      "(2, 0) Policy Evaluation: 8.036 \tMonte-Carlo: 7.319 \tTD(0): 9.036\n",
      "(2, 1) Policy Evaluation: 8.652 \tMonte-Carlo: 8.688 \tTD(0): 9.493\n",
      "(2, 2) Policy Evaluation: 9.568 \tMonte-Carlo: 8.660 \tTD(0): 10.621\n",
      "(2, 3) Policy Evaluation: 10.406 \tMonte-Carlo: 9.870 \tTD(0): 9.992\n",
      "(3, 0) Policy Evaluation: 15.229 \tMonte-Carlo: 12.649 \tTD(0): 15.955\n",
      "(3, 1) Policy Evaluation: 18.674 \tMonte-Carlo: 19.028 \tTD(0): 17.693\n",
      "(3, 2) Policy Evaluation: 23.562 \tMonte-Carlo: 23.547 \tTD(0): 24.548\n",
      "(3, 3) Policy Evaluation: 29.999 \tMonte-Carlo: 29.181 \tTD(0): 30.000\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(vf_td.keys()):\n",
    "    print(key, 'Policy Evaluation: {:0.3f} \\tMonte-Carlo: {:0.3f} \\tTD(0): {:0.3f}'.format(vf_pe[key], vf_mc[key], vf_td[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof that a Fixed Learning Rate For MC is Equivalent to Exponentially Decaying Average of Episode Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
