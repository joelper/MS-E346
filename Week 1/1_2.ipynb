{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write up and code for Assignment 2 Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "#### Markov Process\n",
    "A Markov Process (MP) is a stochastic process where each state follow the Markov property, i.e. $\\mathbb P[S_{t+1}=s'~|~S_1, S_2, ..., S_t] = \\mathbb P[S_{t+1} = s' ~|~ S_t]$. This means that history beyond the current state offers no more information about the transition to the next state. The transition probabilities are governed by a transition probability matrix $\\mathcal P$. The states in an MP consists of a finite set $\\mathcal S$. An MP is then defined by its parameters $\\mathcal S$ and $\\mathcal P$.\n",
    "\n",
    "#### Markov Reward Process\n",
    "In addition to a finite set of states $\\mathcal S$, and a transition probability matrix $\\mathcal P$, a Markov Reward Process (MRP) also has a reward function $\\mathcal R$, ($\\mathcal R_s = \\mathbb E[R_{t+1}~|~S_t = s]$), and a discount factor $\\gamma \\in [0,1]$. An MRP is then defined by the parameters $\\mathcal S$, $\\mathcal P$, $\\mathcal R$ and $\\gamma$. An MRP is thus an extension of an MP.\n",
    "\n",
    "#### Markov Decision Process\n",
    "A Markov Decision Process (MDP) consists of a finite set of states $\\mathcal S$, a transition probability matrix $\\mathcal P$ (that now also depends on the action taken), a reward function $\\mathcal R$, a discount factor $\\gamma$, and a finite set of actions $\\mathcal A$. An MDP is then defined by the parameters $\\mathcal S$, $\\mathcal A$, $\\mathcal P$, $\\mathcal R$ and $\\gamma$. An MDP is thus an extension of an MRP.\n",
    "\n",
    "#### Value function\n",
    "First we define the return $G_t$ as the total discounted rewards recieved starting at time $t$. $$G_t = R_{t} + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + ...$$\n",
    "\n",
    "The state value function $v(s)$ is the value of an MDP when starting in state $s$. It is given by:\n",
    "$$v(s) = \\mathbb E[G_t~|~S_t = s]$$\n",
    "Using the definition of the return we can write the value function as $$v(s) = \\mathbb E[R_t + \\gamma R_{t+1} + ... ~|~S_t = s] = \\mathbb E[R_t ~|~ S_t = s] + \\gamma \\mathbb E[G_{t+1}~|~S_t = s]$$\n",
    "\n",
    "\n",
    "### Data Structures\n",
    "Below is the code for how to represent the above processes in code as structs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Any, Dict, Tuple, Set\n",
    "import numpy as np\n",
    "\n",
    "class MP(NamedTuple):\n",
    "    S: Set[any]\n",
    "    P: Dict[any, Dict[any, float]]\n",
    "        \n",
    "        \n",
    "class MRP(NamedTuple):\n",
    "    # assumes that the reward is just a function of the current state\n",
    "    mp: MP\n",
    "    R: Dict[any, float]\n",
    "    gamma: float\n",
    "    \n",
    "    \n",
    "class MDP(NamedTuple):\n",
    "    S: Set[any]\n",
    "    # the transitions depend on s, a, and s'\n",
    "    # mapping from a state to a mapping of an action to a mapping of a state to a float (probability)\n",
    "    P: Dict[any, Dict[any, Dict[any, float]]]\n",
    "    A: any\n",
    "    R: Dict[any, float]\n",
    "    gamma: float\n",
    "        \n",
    "        \n",
    "class Policy(NamedTuple):\n",
    "    pi: Dict[any, any]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Functions\n",
    "The reward function for a particular movement can be defined in mulitple ways, it can either be a function of the state the Agent leaves and the new state the Agent enters, i.e. $R(s,s')$. Or the reward function can be a function of simply the current state, i.e. $R(s)$. The latter reward function is then simply an expected value of the reward for the next transition: $$R(s) = \\mathbb E[R(s,s')~|~ S = s] = \\sum_{s'\\in \\mathcal S}P(S_{t+1} = s'~|~S_t=s)\\times R(s, s')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_reward(R: Dict[Tuple[any, any], float], P: Dict[any, Dict[any, float]]) -> Dict[any, float]:\n",
    "    # this function converts reward as a function of the transition to reward as a function of the current state\n",
    "    new_R: Dict[any, float]\n",
    "    for s, d in P:\n",
    "        # assume that the first element in the key tuple is the current state\n",
    "        if s not in new_R:\n",
    "            new_R[s] = 0\n",
    "        for sp, p in d:\n",
    "            new_R[s] += p * R[(s, sp)]\n",
    "        \n",
    "    return new_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary Distribution\n",
    "Below is code to find the stationary distribution of a Markov Process. The stationary distirbution can be found analytically using eigenvalues and eigenvectors of the transition matrix. In this problem I've chosen to find the stationary distribution of a Markov Process using simulation. By simulating the process a large number of times, and to a sufficient depth and then count how many times we end up in every state we can estimate the stationary distribution. For a large set of states we need a large number of simulations to get a reasonable estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_stationary_dist(mp: MP, n_iter: int, depth: int) -> Dict[any, float]:\n",
    "    # this function finds a stationary distribution for a Markov Process through simulation\n",
    "    stat_dist = Dict[any, float]\n",
    "    for i in range(n_iter):\n",
    "        state = random.choice(mp.S)\n",
    "        for j in range(depth):\n",
    "            new_state = get_random_state(mp.P[state])\n",
    "            state = new_state\n",
    "        \n",
    "        if state not in stat_dist:\n",
    "            stat_dist[state] = 0\n",
    "        stat_dist[state] += 1/n_iter\n",
    "    return stat_dist\n",
    "\n",
    "\n",
    "def get_random_state(d: Dict[any, float]) -> any:\n",
    "    # helper function to return a random state based on a mapping from a state to a float\n",
    "    p = np.random.rand()\n",
    "    agg_prob = 0\n",
    "    for sp, prob in d:\n",
    "        agg_prob += prob\n",
    "        if p <= agg_prob:\n",
    "            return sp\n",
    "    # degenerate distribution if it does not return before reaching this point\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I've used the eigenvalues approach to finding the stationary distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import eig\n",
    "\n",
    "def get_stat_dist_eig(mp: MP) -> Dict[any, float]:\n",
    "    # create a matrix to store the transition probabilities\n",
    "    mat = np.zeros((len(mp.S), len(mp.S)))\n",
    "#     for i, s, d in enumerate(mp.P):\n",
    "#         if s not in state_to_index:\n",
    "#             state_to_index[s] = i\n",
    "#         for j, sp, p in enumerate(d):\n",
    "#             if sp not in state_to_index:\n",
    "#                 state_to_index[sp] = j\n",
    "    for i, s in enumerate(mp.S):\n",
    "        for j, sp in enumerate(mp.S):\n",
    "            if sp in mp.P[s]:\n",
    "                # if we do not have a mapping from s to sp to a probability the transition has zero probability\n",
    "                mat[i,j] = mp.P[s][sp]\n",
    "    v = get_valid_eig(mat)\n",
    "    dist = Dic\n",
    "    return mat\n",
    "\n",
    "\n",
    "def get_valid_eig(mat: np.array) -> np.array:\n",
    "    # helper function to return a normalized valid eigenvector (i.e. no negative numbers)\n",
    "    eig_vals, eig_vecs = eig(mat.T)\n",
    "    v = np.zeros((eig_vals.shape))\n",
    "    # we use the eigenvector associated with an eigenvalue = 1\n",
    "    for i, val in enumerate(eig_vals):\n",
    "        if np.abs(val - 1.) < 10 ** (-5):\n",
    "            v = eig_vecs[:, i]\n",
    "            break\n",
    "    return v / sum(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
